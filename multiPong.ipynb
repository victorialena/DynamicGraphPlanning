{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e8943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e2e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/victorialena/rlkit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4f41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as gnn # cuda issue?\n",
    "\n",
    "from gym.spaces import Box, MultiDiscrete\n",
    "from numpy.random import rand\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "from replay_buffer import anyReplayBuffer\n",
    "from rlkit.policies.base import Policy\n",
    "\n",
    "\n",
    "from path_collector import MdpPathCollector\n",
    "from replay_buffer import replayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb9a1d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.multiPong import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56e0cba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victorialena/anaconda3/envs/rlpyt/lib/python3.7/site-packages/gym/utils/seeding.py:156: DeprecationWarning: \u001b[33mWARN: Function `create_seed(a, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  \"Function `create_seed(a, max_bytes)` is marked as deprecated and will be removed in the future. \"\n",
      "/home/victorialena/anaconda3/envs/rlpyt/lib/python3.7/site-packages/gym/utils/seeding.py:176: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
      "  \"Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \"\n"
     ]
    }
   ],
   "source": [
    "env = multiPong()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda85432",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9153a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "s = env.reset(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a4dfde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agent, c_in = s.shape # 4, 38 # input feature 2+2+30+2+2\n",
    "c_out = 6 #4 # action space\n",
    "d = 0. # 0.1\n",
    "\n",
    "def get_model():\n",
    "#     return gnn.Sequential('x, edge_index',\n",
    "#                           [(gnn.GATv2Conv(c_in, 16, heads=4, concat=True, dropout=d), 'x, edge_index -> x'), \n",
    "#                             nn.ReLU(inplace=True),\n",
    "#                             (gnn.GATv2Conv(64, 8, heads=4, concat=True, dropout=d), 'x, edge_index -> x'),\n",
    "#                             nn.ReLU(inplace=True),\n",
    "#                             nn.Linear(32, c_out)])\n",
    "#     return gnn.Sequential('x, edge_index',\n",
    "#                           [(gnn.GATv2Conv(c_in, 8, heads=4, concat=True, dropout=d), 'x, edge_index -> x'), \n",
    "#                             nn.ReLU(inplace=True),\n",
    "#                             nn.Linear(32, c_out)])\n",
    "    return nn.Sequential(nn.Linear(c_in, 32),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(32, c_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4624618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class epsilonGreedyPolicy(nn.Module, Policy):\n",
    "    def __init__(self, qf, space, eps=0.1):\n",
    "        super().__init__()\n",
    "        self.qf = qf\n",
    "        self.aspace = space\n",
    "        self.eps = np.clip(eps, .0, 1.)\n",
    "\n",
    "    def __call__(self, x, g):\n",
    "        if rand() < self.eps:\n",
    "            return torch.Tensor(self.aspace.sample()).to(torch.long)\n",
    "        q_values = self.qf(x) #, g)\n",
    "        return q_values.argmax(-1)\n",
    "    \n",
    "    def get_action(self, obs):\n",
    "        return self(obs.x, obs.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2d63baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, agent, max_path_length): #=np.inf):        \n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    terminals = []\n",
    "    next_observations = []\n",
    "    \n",
    "    path_length = 0\n",
    "    o = env.reset()\n",
    "    while env.isterminal():\n",
    "        o = env.reset()\n",
    "        \n",
    "    while path_length < max_path_length:\n",
    "        a = agent(o, env.g)\n",
    "        next_o, r, done, env_info = env.step(copy.deepcopy(a))\n",
    "        \n",
    "        observations.append(o)\n",
    "        rewards.append(r)\n",
    "        terminals.append(done)\n",
    "        actions.append(a)\n",
    "        next_observations.append(next_o)\n",
    "        \n",
    "        path_length += 1\n",
    "        if done:\n",
    "            break\n",
    "        o = next_o\n",
    "        \n",
    "    return dict(\n",
    "        observations=observations,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        next_observations=next_observations,\n",
    "        terminals=terminals,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac985f1",
   "metadata": {},
   "source": [
    "```python\n",
    "# o = env.reset()\n",
    "for _ in range(1):\n",
    "    a = expl_policy(o, env.g)\n",
    "    print(a)\n",
    "    next_o, r, done, env_info = env.step(a)\n",
    "    print(r)\n",
    "    \n",
    "env.render()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "375f5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "scientific_notation =  lambda x:\"{:.2e}\".format(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "562491df",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = get_model()\n",
    "expl_policy = epsilonGreedyPolicy(qf, MultiDiscrete([c_out]*4), 0.1)\n",
    "\n",
    "target_qf = get_model()\n",
    "eval_policy = epsilonGreedyPolicy(target_qf, MultiDiscrete([c_out]*4), 0.0)\n",
    "\n",
    "expl_path_collector = MdpPathCollector(env, expl_policy, rollout_fn=rollout, parallelize=False)\n",
    "eval_path_collector = MdpPathCollector(env, eval_policy, rollout_fn=rollout, parallelize=False)\n",
    "\n",
    "replay_buffer_cap = 4000 #10000\n",
    "replay_buffer = replayBuffer(replay_buffer_cap, prioritized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d23fd469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps = expl_path_collector.collect_nsteps(200, 200, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a25e70",
   "metadata": {},
   "source": [
    "```python\n",
    "path = rollout(env, expl_policy, 2500)\n",
    "path['terminals'][-1]\n",
    "env.render()\n",
    "\n",
    "replay_buffer.add_path(path, env.g)\n",
    "\n",
    "replay_buffer.random_batch(50)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8c0b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(qf.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "max_len = 200\n",
    "n_samples = 1000 #min(32, replay_buffer_cap//max_len) \n",
    "\n",
    "loss = []\n",
    "avg_r_train = []\n",
    "avg_r_test = []\n",
    "max_r_test = []\n",
    "success_rate = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1dd7ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 40\n",
    "n_iter = 64\n",
    "batch_size = 64\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fd3def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b66ae28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reward(paths):\n",
    "    return np.hstack([torch.vstack(p['rewards']).sum(1).numpy() for p in paths]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4bfe025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  -> loss: 0.03621449 | rewards: (train) -0.2363\n",
      "epoch 2  -> loss: 0.02284621 | rewards: (train) -0.17189997\n",
      "epoch 3  -> loss: 0.02209072 | rewards: (train) -0.1302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victorialena/DynamicGraphPlanning/env/multiPong.py:37: RuntimeWarning: Mean of empty slice.\n",
      "  return np.stack([np.stack(np.where(masked_obs == c)).mean(1) for c in summed_cvalues])\n",
      "/home/victorialena/anaconda3/envs/rlpyt/lib/python3.7/site-packages/numpy/core/_methods.py:182: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4  -> loss: 0.02406532 | rewards: (train) -0.191\n",
      "epoch 5  -> loss: 0.02104258 | rewards: (train) -0.097\n",
      "epoch 6  -> loss: 0.02522908 | rewards: (train) -0.17193606\n",
      "epoch 7  -> loss: 0.03069505 | rewards: (train) -0.20631868\n",
      "epoch 8  -> loss: 0.02810762 | rewards: (train) -0.1491\n",
      "epoch 9  -> loss: 0.02967692 | rewards: (train) -0.11960001\n",
      "epoch 10  -> loss: 0.03800928 | rewards: (train) -0.30898204\n",
      "epoch 11  -> loss: 0.02692761 | rewards: (train) -0.14930001\n",
      "epoch 12  -> loss: 0.02880292 | rewards: (train) -0.12200001\n",
      "epoch 13  -> loss: 0.02750982 | rewards: (train) -0.1446\n",
      "epoch 14  -> loss: 0.0205277 | rewards: (train) -0.0909\n",
      "epoch 15  -> loss: 0.02474472 | rewards: (train) -0.1103\n",
      "epoch 16  -> loss: 0.0232911 | rewards: (train) -0.11400001\n",
      "epoch 17  -> loss: 0.01993873 | rewards: (train) -0.1517\n",
      "epoch 18  -> loss: 0.02022458 | rewards: (train) -0.09090001\n",
      "epoch 19  -> loss: 0.0215789 | rewards: (train) -0.1258\n",
      "epoch 20  -> loss: 0.02048987 | rewards: (train) -0.1071\n",
      "epoch 21  -> loss: 0.01811218 | rewards: (train) -0.0988\n",
      "epoch 22  -> loss: 0.02634228 | rewards: (train) -0.1471\n",
      "epoch 23  -> loss: 0.02333275 | rewards: (train) -0.0995\n",
      "epoch 24  -> loss: 0.02614759 | rewards: (train) -0.13939999\n",
      "epoch 25  -> loss: 0.02584356 | rewards: (train) -0.18390001\n",
      "epoch 26  -> loss: 0.02758832 | rewards: (train) -0.18711925\n",
      "epoch 27  -> loss: 0.03312838 | rewards: (train) -0.16863678\n",
      "epoch 28  -> loss: 0.02917431 | rewards: (train) -0.12613112\n",
      "epoch 29  -> loss: 0.03049494 | rewards: (train) -0.11579999\n",
      "epoch 30  -> loss: 0.02838944 | rewards: (train) -0.12532866\n",
      "epoch 31  -> loss: 0.02843834 | rewards: (train) -0.12859999\n",
      "epoch 32  -> loss: 0.02584746 | rewards: (train) -0.1066\n",
      "epoch 33  -> loss: 0.02257983 | rewards: (train) -0.1268\n",
      "epoch 34  -> loss: 0.03173307 | rewards: (train) -0.23095918\n",
      "epoch 35  -> loss: 0.02930366 | rewards: (train) -0.0903\n",
      "epoch 36  -> loss: 0.02616489 | rewards: (train) -0.0818\n",
      "epoch 37  -> loss: 0.01969902 | rewards: (train) -0.0529\n",
      "epoch 38  -> loss: 0.01448443 | rewards: (train) -0.1191\n",
      "epoch 39  -> loss: 0.01532857 | rewards: (train) -0.0936255\n",
      "epoch 40  -> loss: 0.01538484 | rewards: (train) -0.0794\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_epoch):\n",
    "    qf.train(False)\n",
    "    paths = expl_path_collector.collect_nsteps(n_samples, max_len, False)\n",
    "    train_r = mean_reward(paths)\n",
    "    replay_buffer.add_paths(paths, env.g)\n",
    "\n",
    "    qf.train(True)\n",
    "    for _ in range(n_iter): \n",
    "        batch = replay_buffer.random_batch(batch_size)\n",
    "\n",
    "        rewards = batch.r.to(torch.float)\n",
    "        terminals = batch.t.to(torch.float).repeat_interleave(4)\n",
    "        actions = batch.a\n",
    "        states = batch.x\n",
    "        next_s = batch.next_s\n",
    "\n",
    "        out = target_qf(next_s) #, env.g)\n",
    "        target_q_values = out.max(-1, keepdims=False).values\n",
    "        y_target = rewards + (1. - terminals) * gamma * target_q_values \n",
    "        \n",
    "        out = qf(states) #, env.g)\n",
    "        actions_one_hot = F.one_hot(actions.to(torch.int64), c_out)\n",
    "        y_pred = torch.sum(out * actions_one_hot, dim=-1, keepdim=False)\n",
    "        qf_loss = qf_criterion(y_pred, y_target).to(torch.float)\n",
    "\n",
    "        loss.append(qf_loss.item())\n",
    "        avg_r_train.append(rewards.mean().item())\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        qf_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    target_qf.load_state_dict(deepcopy(qf.state_dict()))\n",
    "    err = 8\n",
    "    print(\"epoch\", i+1, #\"| lr:\", scientific_notation(optimizer.param_groups[0][\"lr\"]) ,\n",
    "          \" -> loss:\", round(np.mean(loss[-n_iter:]), err),\n",
    "          \"| rewards: (train)\", round(train_r, err))\n",
    "#           \"| (test)\", round(avg_r_test[-1], err), \"| (max)\", round(max_r_test[-1], err),\n",
    "#           \"| success rate:\", round(success_rate[-1], err))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192c4bb8",
   "metadata": {},
   "source": [
    "#### notes:\n",
    "\n",
    "1. reduce feature space\n",
    "    b. reduce action space\n",
    "2. try linear network\n",
    "3. double check sample trajectories\n",
    "4. check sampled batches\n",
    "5. remove \"grey\" from reward structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321f9eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlpyt] *",
   "language": "python",
   "name": "conda-env-rlpyt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
