{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75555ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c38d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/victorialena/rlkit')\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae7dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from copy import copy, deepcopy\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09210053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.job_shop import jobShopScheduling\n",
    "from actor.job_shop import *\n",
    "from utils.job_shop import * # custom replay buffer and episode sampler\n",
    "\n",
    "from path_collector import MdpPathCollector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65024a",
   "metadata": {},
   "source": [
    "#### elpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a76a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "scientific_notation =  lambda x:\"{:.2e}\".format(x)\n",
    "\n",
    "def get_scores(g, scores):\n",
    "    n = scores.shape[0]\n",
    "    idx = (g.ndata['hv']['job'][:, 3] == 0).view(n, -1)\n",
    "    \n",
    "    values, workers = scores.max(-1, keepdims=False)\n",
    "    return torch.stack([values[i][idx[i]].max() if sum(idx[i]).item()>0 else torch.tensor(0.) for i in range(n)])\n",
    "\n",
    "def mean_reward(paths):\n",
    "    return torch.tensor([p['rewards'] for p in paths]).sum(1).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e89c0ef",
   "metadata": {},
   "source": [
    "#### Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffee97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e901c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "njobs, nworkers = 5, 2\n",
    "env = jobShopScheduling(njobs, nworkers)\n",
    "g0 = env.reset()\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a96dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = hgnn()\n",
    "expl_policy = epsilonGreedyPolicy(qf, .1)\n",
    "\n",
    "target_qf = hgnn()\n",
    "eval_policy = epsilonGreedyPolicy(target_qf, 0.)\n",
    "\n",
    "expl_path_collector = MdpPathCollector(env, expl_policy, rollout_fn=sample_episode, parallelize=False)\n",
    "eval_path_collector = MdpPathCollector(env, eval_policy, rollout_fn=sample_episode, parallelize=False)\n",
    "\n",
    "replay_buffer_cap = 2000 #10000\n",
    "replay_buffer = replayBuffer(replay_buffer_cap, prioritized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37a30c0",
   "metadata": {},
   "source": [
    "```python\n",
    "path = rollout(env, expl_policy, 2500)\n",
    "path['terminals'][-1]\n",
    "env.render()\n",
    "\n",
    "replay_buffer.add_path(path, env.g)\n",
    "\n",
    "replay_buffer.random_batch(50)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(qf.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "qf_criterion = nn.MSELoss()\n",
    "\n",
    "max_len = njobs+1\n",
    "n_samples = 128 \n",
    "n_epoch = 200\n",
    "n_iter = 64\n",
    "batch_size = 32\n",
    "gamma = 1.0\n",
    "\n",
    "loss = []\n",
    "avg_r_train = []\n",
    "avg_r_eval = []\n",
    "success_rates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72563023",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_epoch):\n",
    "    qf.train(False)\n",
    "    paths = expl_path_collector.collect_new_paths(n_samples, max_len, False)\n",
    "    train_r = mean_reward(paths)\n",
    "    avg_r_train.append(train_r)\n",
    "    replay_buffer.add_paths(paths)\n",
    "    \n",
    "    paths = eval_path_collector.collect_new_paths(n_samples//4, max_len, False)\n",
    "    eval_r = mean_reward(paths)\n",
    "    avg_r_eval.append(eval_r)\n",
    "    \n",
    "    success_rate = np.mean([p['success'] for p in paths])\n",
    "    success_rates.append(success_rate)\n",
    "\n",
    "    qf.train(True)\n",
    "    for _ in range(n_iter):\n",
    "        batch = replay_buffer.random_batch(batch_size)\n",
    "\n",
    "        rewards = torch.tensor([b.r for b in batch])\n",
    "        terminals =torch.tensor([b.d for b in batch]).float()\n",
    "        actions = torch.tensor([b.a for b in batch])\n",
    "        \n",
    "        states = batch_graphs([b.s for b in batch])\n",
    "        next_s = batch_graphs([b.sp for b in batch])        \n",
    "\n",
    "        out = target_qf(next_s) # shape = (|G|, |J|, |W|)\n",
    "        target_q_values = get_scores(next_s, out)\n",
    "        y_target = rewards + (1. - terminals) * gamma * target_q_values \n",
    "        \n",
    "        out = qf(states)\n",
    "        y_pred = out[torch.arange(batch_size), actions.T[1], actions.T[0]]\n",
    "        qf_loss = qf_criterion(y_pred, y_target).to(torch.float)\n",
    "\n",
    "        loss.append(qf_loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    target_qf.load_state_dict(deepcopy(qf.state_dict()))\n",
    "    err = 8\n",
    "    print(\"epoch\", i+1, #\"| lr:\", scientific_notation(optimizer.param_groups[0][\"lr\"]) ,\n",
    "          \" -> loss:\", round(np.mean(loss[-n_iter:]), err),\n",
    "          \"| rewards: (train)\", round(train_r, err), \"(test)\", round(eval_r, err),\n",
    "          \"| success rate:\", round(success_rate, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0df25b",
   "metadata": {},
   "source": [
    "```python\n",
    "g1 = env.reset()\n",
    "qf.eval()\n",
    "print(g1.edges(etype='precede'))\n",
    "print(qf(g1))\n",
    "\n",
    "g2, r, d, info = env2.step((0, 1))\n",
    "\n",
    "# paths = eval_path_collector.collect_new_paths(10, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3416030c",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [np.mean(loss[i*n_iter:(i+1)*n_iter]) for i in range(n_epoch)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc46ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(n_epoch)\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.plot(x, losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.subplot(132)\n",
    "plt.plot(x, avg_r_train, label=\"test\")\n",
    "plt.plot(x, avg_r_eval, 'r--', label=\"eval\")\n",
    "plt.legend()\n",
    "plt.ylabel('Train/Test Rewards [path, avg]')\n",
    "plt.xlabel('Epoch')\n",
    "plt.subplot(133)\n",
    "plt.plot(x, success_rates)\n",
    "plt.ylabel('Success Rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.suptitle('Training Performance Summary')\n",
    "plt.savefig('figs/job_shop/j%d-w%d_hor' % (njobs, nworkers), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719a7dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ylabels = ['Loss', 'Train/Test Rewards', 'Success Rate']\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(6, 12))\n",
    "plt.suptitle('Training Performance Summary', y=.92)\n",
    "axs[0].plot(x, losses)\n",
    "axs[1].plot(x, avg_r_train, label=\"train\")\n",
    "axs[1].plot(x, avg_r_eval, 'r--', label=\"eval\")\n",
    "axs[1].legend()\n",
    "axs[2].plot(x, success_rates)\n",
    "\n",
    "for label, ax in zip(ylabels, axs.flat):\n",
    "    ax.set(xlabel='Epoch', ylabel=label)\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "plt.savefig('figs/job_shop/j%d-w%d_ver' % (njobs, nworkers), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c26665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlpyt] *",
   "language": "python",
   "name": "conda-env-rlpyt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
