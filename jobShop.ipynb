{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69a6b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73bb58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f83c1317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import MultiDiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df73b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy, deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa7995a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victorialena/anaconda3/envs/rlpyt/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/home/victorialena/anaconda3/envs/rlpyt/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Sized\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76da2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_queues(n: int, beta: float = 0.8):\n",
    "    free = [True]*n\n",
    "    out = []\n",
    "    for i in np.random.permutation(n):\n",
    "        if not free[i]:\n",
    "            continue\n",
    "        free[i] = False\n",
    "        \n",
    "        while np.random.rand() < beta:\n",
    "            try:\n",
    "                j = np.random.choice(np.where(free)[0])\n",
    "            except:\n",
    "                return out\n",
    "            free[j] = False\n",
    "            out.append([i, j])\n",
    "            i = j\n",
    "    return out\n",
    "\n",
    "def count_q_length(_from, _to, n):\n",
    "    counts, prev_counts = torch.zeros(n), torch.zeros(n)\n",
    "    counts[_from] = 1\n",
    "    while not all(counts == prev_counts):\n",
    "        prev_counts = deepcopy(counts)\n",
    "        counts[_from] = counts[_to]+1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86320da5",
   "metadata": {},
   "source": [
    "```python\n",
    "g.nodes('job')\n",
    "g.ntypes\n",
    "g.etypes\n",
    "g.canonical_etypes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada9a735",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dcd4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class jobShopScheduling(gym.Env):\n",
    "    \"\"\"\n",
    "    ### Description\n",
    "    Learning to schedule a successful sequence of “job” to multiple workers respecting given constraints. \n",
    "    \n",
    "    ### Action Space\n",
    "    By adding an edge from a worker to an unscheduled job, the job gets queued to that thread.\n",
    "    The resulting sequence can not be chnaged in hindsight.\n",
    "    \n",
    "    ### State Space    \n",
    "    A disjunctive heterogeneous graph g = (V, C U D). Each node represents a “job” or a “worker”. \n",
    "    Edges in C denote succession requirements for jobs, edges in D denotes which jobs were assigned to \n",
    "    which worker. \n",
    "    \n",
    "    ### Rewards\n",
    "    The system recieves a positive unit reward for each executed job. And a penalty per time step.\n",
    "    \n",
    "    ### Starting State\n",
    "    A random set of n jobs, including time requirements and succession constraints, e.g., task i requires \n",
    "    completion of task j.\n",
    "    \n",
    "    ### Episode Termination\n",
    "    The episode terminates when all jobs have been scheduled. Then the action space has schunken to size 0.\n",
    "    The final reward tallies up the remaining rewards to be versed (w/o time discounting).\n",
    "    \n",
    "    ### Arguments\n",
    "    No additional arguments are currently supported.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, njobs: int, nworkers: int):\n",
    "        self._njobs = njobs\n",
    "        self._nworkers = nworkers\n",
    "        self._jfeat = 7\n",
    "        self._wfeat = 3\n",
    "        self._dt = 0.1\n",
    "        self._time_penalty = -0.01\n",
    "        \n",
    "        self._state = None\n",
    "        \n",
    "    def reward(self, a):\n",
    "        assert False, \"Not implemented. Do not call.\"\n",
    "    \n",
    "    def terminal(self):\n",
    "        # Terminal state is reached when all the jobs have been scheduled. |A| is zero.\n",
    "        return all(self._state.nodes['job'].data['hv'][:, 3] == 1)\n",
    "    \n",
    "    def worker_features(self):\n",
    "        return ('n queued', 'expected run time', 'efficiency rate')\n",
    "    \n",
    "    def job_features(self):\n",
    "        return ('time req', \n",
    "                'completion%', #1\n",
    "                'nr of child nodes', #2 \n",
    "                'status (one hot: scheduled, processing, finished)', #3-4-5\n",
    "                'remaining time') #6\n",
    "    \n",
    "    def valid_action(self, a):\n",
    "        _, j = a\n",
    "        return self._state.nodes['job'].data['hv'][j, 3] == 0\n",
    "    \n",
    "    def check_job_requirements(self, j):\n",
    "        # Return True if no incoming edges from preceding job requirements.\n",
    "        _, dst = self._state.edges(etype='precede')\n",
    "        return all(dst != j)\n",
    "    \n",
    "    def rollout(self, verbose=False):\n",
    "        # Return number of jobs complete if we just waited until all workers exit (done of gridlock)\n",
    "        # Does not take into account discount factor!\n",
    "        state_hv = deepcopy(self._state.nodes['job'].data['hv'])\n",
    "        state_he = deepcopy(self._state.nodes['worker'].data['he'])\n",
    "        \n",
    "        jdone = state_hv[:, 5] == 1\n",
    "        \n",
    "        reward = torch.tensor([0.])\n",
    "        src, dst = deepcopy(self._state.edges(etype='processing'))\n",
    "        sreq, dreq = deepcopy(self._state.edges(etype='precede'))\n",
    "        \n",
    "        while True:\n",
    "            idx = [dst[src==w][0].item() for w in src.unique().tolist()]\n",
    "            idx = [j for j in idx if all(jdone[sreq[dreq==j]])]\n",
    "            if len(idx) == 0:\n",
    "                break # gridlock\n",
    "            \n",
    "            # get smallest remaining time for idx. -(.dt)\n",
    "            j = idx[state_hv[idx, 6].argmin().item()]\n",
    "            if verbose:\n",
    "                print(\"executing job\", j, \"on worker\", src[dst==j].item())\n",
    "            jdone[j] = True\n",
    "            reward += 1. + state_hv[j, 6].div(self._dt, rounding_mode='trunc')*self._time_penalty\n",
    "            state_hv[idx, 6] -= state_hv[j, 6] #mark that job as done\n",
    "            \n",
    "            # remove job from queue\n",
    "            src = src[dst!=j]\n",
    "            dst = dst[dst!=j]\n",
    "            \n",
    "        # clean up graph\n",
    "        idx = np.where(jdone)[0]\n",
    "        \n",
    "        src, dst, cnts = self._state.edges('all', etype='processing')\n",
    "        jidx = [(j in idx) for j in dst]\n",
    "        self._state.remove_edges(cnts[jidx].tolist(), 'processing')                \n",
    "        \n",
    "        for etype in ['next', 'precede']:\n",
    "            src, dst, cnts = self._state.edges('all', etype=etype)\n",
    "            jidx = [(j in idx) for j in src]\n",
    "            self._state.remove_edges(cnts[jidx].tolist(), etype)\n",
    "            \n",
    "        self._state.nodes['job'].data['hv'][idx, 2] = 1.\n",
    "        self._state.nodes['job'].data['hv'][idx, 4] = 0.\n",
    "        self._state.nodes['job'].data['hv'][idx, 5] = 1. # mark terminal\n",
    "        self._state.nodes['job'].data['hv'][idx, 6] = 0. # set remaining time to 0\n",
    "        \n",
    "        src, _ = self._state.edges(etype='processing')\n",
    "        if len(src):\n",
    "            w, cnts = src.unique(return_counts=True)\n",
    "            self._state.nodes['worker'].data['he'][w, 0] = cnts.float()\n",
    "\n",
    "        return reward.item(), all(jdone)\n",
    "    \n",
    "    def get_node_status(self, label, by_index=True):\n",
    "        mask = self._state.nodes['job'].data['hv'][:, label] == 1\n",
    "        if by_index:\n",
    "            return np.where(mask)[0].tolist()\n",
    "        return mask\n",
    "        \n",
    "    def get_scheduled(self, by_index=True):\n",
    "        return self.get_node_status(3, by_index)\n",
    "    \n",
    "    def get_unscheduled(self, by_index=True):\n",
    "        mask = ~self.get_node_status(3, False)\n",
    "        if by_index:\n",
    "            return np.where(mask)[0].tolist()\n",
    "        return mask\n",
    "    \n",
    "    def get_processing(self, by_index=True):\n",
    "        return self.get_node_status(4, by_index)\n",
    "    \n",
    "    def get_terminated(self, by_index=True):\n",
    "        return self.get_node_status(5, by_index)\n",
    "    \n",
    "    def is_gridlocked(self):\n",
    "        if len(self.get_processing()):\n",
    "            return False\n",
    "\n",
    "        src, dst = self._state.edges(etype='processing')\n",
    "        if len(src.unique()) != self._nworkers:\n",
    "            return False\n",
    "\n",
    "        _, req = self._state.edges(etype='precede')\n",
    "        newidx = [dst[src==w][0].item() for w in src.unique().tolist()]\n",
    "        newidx = [j for j in newidx if j not in req]\n",
    "        return len(newidx)>0\n",
    "    \n",
    "    def step(self, a):\n",
    "        assert self.valid_action(a), \"Invalid action taken: (w:%d, j:%d)\" % a\n",
    "        \n",
    "        src, dst, cnts = self._state.edges('all', etype='processing')\n",
    "        \n",
    "        \"\"\" \n",
    "        1) Schedule job j for worker w: \n",
    "            a) Find last job scheduled for worker w, add edge from end of queue to new job j. \n",
    "            b) Add edge from w to j. \n",
    "            c) Update worker info (queue length, run time estimate).\n",
    "            d) Mark job as scheduled.\n",
    "        \"\"\"\n",
    "        w, j = a\n",
    "        if w in src:\n",
    "            _i = dst[src==w][-1].item() # add to end of q -- last edge added\n",
    "            self._state.add_edge(_i, j, etype='next')        \n",
    "        self._state.add_edge(w, j, etype='processing')\n",
    "        \n",
    "        state_hv = deepcopy(self._state.nodes['job'].data['hv'])\n",
    "        state_he = deepcopy(self._state.nodes['worker'].data['he'])\n",
    "        \n",
    "        state_he[w, 0] += 1. # add job to work queue length\n",
    "        state_he[w, 1] += state_hv[j, 0] # update worker' run time estimate\n",
    "        state_hv[j, 3] = 1. # mark as scheduled\n",
    "                \n",
    "        \"\"\" 2) Assure the first job in queue is being processed at this time step. \"\"\"\n",
    "        _, req = self._state.edges(etype='precede')\n",
    "        # src, dst, cnts = self._state.edges('all', etype='processing') # call again to update processing\n",
    "        newidx = [dst[src==w][0].item() for w in src.unique().tolist()]\n",
    "        newidx = [j for j in newidx if j not in req]\n",
    "        state_hv[newidx, 4] = 1 # set to processing (but completion % remain 0)\n",
    "        \n",
    "        # write info incase of early exit\n",
    "        self._state.nodes['job'].data['hv'] = state_hv\n",
    "        self._state.nodes['worker'].data['he'] = state_he\n",
    "\n",
    "        \"\"\" \n",
    "        3) Update feature vectors:\n",
    "            a) Progress time for node features: remaining time, completion % for jobs and workers\n",
    "            b) Update info around terminal jobs, and remove processing edge if job has terminated.\n",
    "            c) Remove next and precede edges for terminated jobs. \n",
    "        \"\"\"\n",
    "        # a\n",
    "        processing_mask = state_hv[:, 4] == 1        \n",
    "        if processing_mask.sum() == 0:\n",
    "            return deepcopy(self._state), self._time_penalty, self.terminal(), {'success':False}\n",
    "        \n",
    "        state_hv[processing_mask, 6] = torch.maximum(state_hv[processing_mask, 6]-self._dt,\n",
    "                                                     torch.zeros(processing_mask.sum())).round(decimals=2) # update remaining time\n",
    "        state_hv[processing_mask, 1] = torch.clamp(1-torch.div(state_hv[processing_mask, 6],\n",
    "                                                               state_hv[processing_mask, 0]), \n",
    "                                                   min=0, max=1) # update completion %\n",
    "        \n",
    "        state_he[:, 1] = torch.maximum(state_he[:, 1]-self._dt, torch.zeros(self._nworkers)) # update remaining time\n",
    "        \n",
    "        # b\n",
    "        state_hv[processing_mask, 5] = (state_hv[processing_mask, 1] == 1).float() # mark terminal\n",
    "        state_hv[processing_mask, 4] = 1-state_hv[processing_mask, 5] # if terminal, job no longer processing        \n",
    "        idx = torch.where(processing_mask)[0][torch.where(state_hv[processing_mask, 5])[0]].tolist() # job ids just terminated\n",
    "        if len(idx):\n",
    "            widx = [(j in idx) for j in dst]\n",
    "            state_he[src[widx], 0] -= 1 # remove job from job count\n",
    "            self._state.remove_edges(cnts[widx].tolist(), 'processing') # delete those edges?\n",
    "        \n",
    "            # c\n",
    "            src, dst, cnts = self._state.edges('all', etype='next')\n",
    "            ptridx = torch.tensor([cnts[src == j].item() for j in idx if j in src]) # this works because it is a queue: unique next node\n",
    "            if len(ptridx):\n",
    "                self._state.remove_edges(ptridx.tolist(), 'next')\n",
    "\n",
    "            src, dst, cnts = self._state.edges('all', etype='precede')\n",
    "            jidx = [(j in idx) for j in src]\n",
    "            if len(jidx):\n",
    "                self._state.remove_edges(cnts[jidx].tolist(), 'precede') # delete those edges?\n",
    "                \n",
    "        \"\"\" 5) Update feature vectors. \"\"\"\n",
    "        self._state.nodes['job'].data['hv'] = state_hv\n",
    "        self._state.nodes['worker'].data['he'] = state_he\n",
    "                \n",
    "        \"\"\" 6) Compute reward and terminal state. \"\"\"\n",
    "        done = self.terminal()\n",
    "        success = False\n",
    "        n_terminal = len(idx)\n",
    "        reward = self._time_penalty + n_terminal\n",
    "        \n",
    "        if done:\n",
    "            r, success = self.rollout()\n",
    "            reward += r\n",
    "        \n",
    "        return deepcopy(self._state), deepcopy(reward), deepcopy(done), {'success':success}\n",
    "\n",
    "    def reset(self, seed: int = None, topology: str = 'random'):\n",
    "        if not seed == None:\n",
    "            super().reset(seed=seed)\n",
    "        \n",
    "        nw, nj = self._nworkers, self._njobs\n",
    "        _from, _to = torch.tensor([[0, 0]]+get_random_queues(nj)).T\n",
    "        \n",
    "        graph_data = {\n",
    "           ('job', 'precede', 'job'): (_from, _to), # A ---before---> B\n",
    "           ('job', 'next', 'job'): (torch.tensor([0]), torch.tensor([0])), # jobshop queue\n",
    "           ('worker', 'processing', 'job'): (torch.tensor([0]), torch.tensor([0])) # nothing is scheduled\n",
    "        }\n",
    "        \n",
    "        self._state = dgl.heterograph(graph_data, num_nodes_dict={'worker': nw, 'job': nj})\n",
    "        # hack: can not init null vector for edges\n",
    "        self._state.remove_edges(0, 'processing')\n",
    "        self._state.remove_edges(0, 'precede')\n",
    "        self._state.remove_edges(0, 'next')\n",
    "                \n",
    "        times = 0.1*torch.randint(1, 10, (nj,1)) # torch.rand(nj,1)\n",
    "        _from, _to = _from[1:], _to[1:]\n",
    "        counts = count_q_length(_from, _to, nj).unsqueeze(-1)\n",
    "        self._state.nodes['job'].data['hv'] = torch.cat((times, torch.zeros(nj, 1), counts, torch.zeros(nj, 3), times), 1)\n",
    "        self._state.nodes['worker'].data['he'] = torch.cat((torch.zeros(nw,2), torch.ones(nw,1)), 1)\n",
    "        \n",
    "        return deepcopy(self._state)\n",
    "    \n",
    "    def dump_state_info(self):\n",
    "        print('scheduled:', env.get_scheduled())\n",
    "        print('processing:', env.get_processing())\n",
    "        print('terminated:', env.get_terminated())\n",
    "        print('job data:')\n",
    "        print(self._state.nodes['job'].data['hv'])\n",
    "        print('worker data:')\n",
    "        print(self._state.nodes['worker'].data['he'])\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        import networkx as nx\n",
    "        import matplotlib.pyplot as plt\n",
    "        \"\"\"\n",
    "        self.dump_state_info()\n",
    "        \n",
    "        G = dgl.to_homogeneous(self._state).to_networkx(edge_attrs=['_TYPE'])\n",
    "        \n",
    "        node_color = ['red']*self._njobs+['orange']*self._nworkers\n",
    "        edge_color = ['red' if e[-1].item() % 2 else 'orange' for e in G.edges(data='_TYPE')]\n",
    "        nx.draw(G, node_color=node_color, edge_color=edge_color, with_labels=True)\n",
    "        \n",
    "    def seed(self, n: int):\n",
    "        super().reset(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ab4d81",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dd25fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "import dgl.nn.pytorch as dglnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7a4eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_readout_graph(g, etype):\n",
    "    \"\"\" \n",
    "    Returns graph of edges to be evalutated, i.e., draw edges from each worker to each job node.\n",
    "    If g is a batched graph representation, return edges within each subgraph.\n",
    "    etype = use .canonical_etypes() \n",
    "    \"\"\"\n",
    "    \n",
    "    utype, _, vtype = etype\n",
    "    nu, nv = g.num_nodes(utype), g.num_nodes(vtype)\n",
    "    if not is_batched_graph(g):\n",
    "        src, dst = g.nodes(utype).repeat(nv), g.nodes(vtype).repeat_interleave(nu)\n",
    "        \n",
    "        return dgl.heterograph({etype: (src, dst)},\n",
    "                               num_nodes_dict={utype: nu, vtype: nv})\n",
    "    # else:\n",
    "    node_ids = g.ndata['id']\n",
    "    src = torch.cat([g.nodes(utype)[node_ids[utype]==i].repeat(c)\n",
    "                     for i, c in zip(*node_ids[vtype].unique(return_counts=True))])\n",
    "    dst = torch.cat([g.nodes(vtype)[node_ids[vtype]==i].repeat_interleave(c)\n",
    "                     for i, c in zip(*node_ids[utype].unique(return_counts=True))])\n",
    "    \n",
    "    out = dgl.heterograph({etype: (src, dst)}, num_nodes_dict={utype: nu, vtype: nv})\n",
    "    out.nodes['job'].data['id'] = g.nodes['job'].data['id']\n",
    "    out.nodes['worker'].data['id'] = g.nodes['worker'].data['id']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b97fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_batched_graph(g):\n",
    "    return len(g.ndata['id'])!=0\n",
    "\n",
    "def num_subgraphs(g):\n",
    "    return 1 if not is_batched_graph(g) else len(g.ndata['id']['job'].unique())\n",
    "\n",
    "def sg_nworkers(g):\n",
    "    if is_batched_graph(g):\n",
    "        idx, cnts = g.ndata['id']['worker'].unique(return_counts=True)\n",
    "        if len(cnts.unique()) == 1:\n",
    "            return cnts.unique().item()\n",
    "        return cnts\n",
    "    return g.num_nodes('worker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccda8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotProductPredictor(nn.Module):\n",
    "    \"\"\" returns scores for each job (row) per worker (col)\"\"\"\n",
    "    def forward(self, graph, hv, he, _etype):\n",
    "        # hv contains the node representations computed from the GNN\n",
    "        utype, etype, vtype = _etype\n",
    "        nu = sg_nworkers(graph)\n",
    "        assert type(nu)==int, \"Graphs have different workers counts. Can not proceed this eval without errors.\"\n",
    "        with graph.local_scope():\n",
    "            graph.nodes[vtype].data['hv'] = hv\n",
    "            graph.nodes[utype].data['he'] = he\n",
    "            graph.apply_edges(fn.u_dot_v('he', 'hv', 'score'), etype=etype)\n",
    "            return graph.edges[etype].data['score'].view(num_subgraphs(graph), -1, nu).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f0cd7",
   "metadata": {},
   "source": [
    "```python\n",
    "class agent(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, rel_names):\n",
    "        super().__init__()\n",
    "        self.sage = RGCN(in_features, hidden_features, out_features, rel_names)\n",
    "        self.pred = dotProductPredictor()\n",
    "    \n",
    "    def forward(self, g, x, etype):        \n",
    "        hv = self.sage(g, x)\n",
    "        he = g.nodes['worker'].data['he']\n",
    "        rg = construct_readout_graph(g, ('worker', 'processing', 'job'))\n",
    "        return self.pred(rg, hv, he, ('worker', 'processing', 'job'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d1c6d2",
   "metadata": {},
   "source": [
    "```python\n",
    "from dgl.nn.pytorch.conv import HGTConv\n",
    "layer = HGTConv(7, head_size=16, num_heads=4, num_ntypes=2, num_etypes=3, dropout=0.1, use_norm=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677dddb3",
   "metadata": {},
   "source": [
    "```python\n",
    "env = jobShopScheduling(10, 2)\n",
    "g = env.reset()\n",
    "env.render()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dd8c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hgnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = dglnn.HeteroLinear({'job': 7, 'worker':3}, 16)\n",
    "        self.conv = dglnn.HeteroGraphConv({\n",
    "            'precede' : dglnn.GraphConv(16, 16),\n",
    "            'next' : dglnn.GraphConv(16, 16),\n",
    "            'processing' : dglnn.SAGEConv((16, 16), 16, 'mean')},\n",
    "            aggregate='sum')\n",
    "        self.pred = dotProductPredictor()\n",
    "        \n",
    "    def forward(self, g):\n",
    "        h0 = {**g.ndata['hv'], **g.ndata['he']}\n",
    "        h1 = self.embedding(h0)\n",
    "        hv = self.conv(g, h1)['job']\n",
    "        \n",
    "        rg = construct_readout_graph(g, ('worker', 'processing', 'job'))\n",
    "        return self.pred(rg, hv, h1['worker'], ('worker', 'processing', 'job'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25b59778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_graphs(batch):\n",
    "    n = len(batch)\n",
    "    \n",
    "    njs = torch.tensor([g.num_nodes('job') for g in batch])\n",
    "    nws = torch.tensor([g.num_nodes('worker') for g in batch])\n",
    "    \n",
    "    def concat_edges(etype, idx, ns):\n",
    "        return torch.cat([batch[i].edges(etype=etype)[idx]+ns[:i].sum() for i in range(n)])\n",
    "    \n",
    "    batched_graph_data = {\n",
    "        ('job', 'precede', 'job'): (concat_edges('precede', 0, njs), concat_edges('precede', 1, njs)), \n",
    "        ('job', 'next', 'job'): (concat_edges('next', 0, njs), concat_edges('next', 1, njs)),\n",
    "        ('worker', 'processing', 'job'): (concat_edges('processing', 0, nws), concat_edges('processing', 1, njs)),\n",
    "    }\n",
    "\n",
    "    state = dgl.heterograph(batched_graph_data, \n",
    "                            num_nodes_dict={'worker': nws.sum().item(), 'job': njs.sum().item()})\n",
    "\n",
    "    state.nodes['job'].data['hv'] = torch.cat([g.nodes['job'].data['hv'] for g in batch])\n",
    "    state.nodes['worker'].data['he'] = torch.cat([g.nodes['worker'].data['he'] for g in batch])\n",
    "    state.nodes['job'].data['id'] = torch.arange(n).repeat_interleave(njs)\n",
    "    state.nodes['worker'].data['id'] = torch.arange(n).repeat_interleave(nws)\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cccacff",
   "metadata": {},
   "source": [
    "### Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48c4221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/victorialena/rlkit')\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08bf2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlkit.policies.base import Policy\n",
    "from path_collector import MdpPathCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaf3a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scientific_notation =  lambda x:\"{:.2e}\".format(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74dc1133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class epsilonGreedyPolicy(nn.Module, Policy):\n",
    "    def __init__(self, net, eps=0.1):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.eps = np.clip(eps, .0, 1.)\n",
    "\n",
    "    def __call__(self, g):\n",
    "        return self.get_action(g)\n",
    "    \n",
    "    def get_action(self, g):\n",
    "        idx = np.where(g.ndata['hv']['job'][:, 3] == 0)[0] # idx: list of unscheduled job ids\n",
    "        assert len(idx) > 0, \"Unecessary query. Empty action space.\"\n",
    "        \n",
    "        if np.random.rand() < self.eps:\n",
    "            nw = g.num_nodes(ntype='worker')\n",
    "            return (np.random.randint(nw), np.random.choice(idx))\n",
    "        \n",
    "        out = self.net(g)\n",
    "        val, workers = out[idx].max(1)\n",
    "        j = val.argmax().item()\n",
    "        w = workers[j].item()\n",
    "        return (w, idx[j])\n",
    "    \n",
    "    def get_actions(self, g):\n",
    "        assert is_batched_graph(g), \"This is a single graph, call get_action() instead.\"\n",
    "        \n",
    "        n = num_subgraphs(g)\n",
    "        idx = (g.ndata['hv']['job'][:, 3] == 0).view(n, -1)\n",
    "        assert sum(idx).item() > 0, \"Empty action space!\"\n",
    "        \n",
    "        scores = self.net(g)\n",
    "        out = {}\n",
    "        values, workers = scores.max(-1, keepdims=False)\n",
    "        for i in range(n):\n",
    "            j = torch.where(idx[i])[0][values[i, idx[i]].argmax().item()].item()\n",
    "            out[i] = (workers[i, j].item(), j)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05e47853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episode(env, agent, max_path_length:int=100):\n",
    "    obs = env.reset()\n",
    "    \n",
    "    observations=[]\n",
    "    actions=[]\n",
    "    rewards=[]\n",
    "    next_observations=[]\n",
    "    terminals=[]\n",
    "    \n",
    "    success=False\n",
    "    \n",
    "    for _ in range(max_path_length):\n",
    "        a = agent.get_action(obs)\n",
    "        nextobs, reward, done, info = env.step(a)\n",
    "        \n",
    "        observations.append(obs)\n",
    "        actions.append(deepcopy(a))\n",
    "        rewards.append(reward)\n",
    "        next_observations.append(nextobs)\n",
    "        terminals.append(done)\n",
    "        \n",
    "        obs = nextobs\n",
    "        success = info['success']\n",
    "        \n",
    "        if done: \n",
    "            break\n",
    "            \n",
    "    return dict(\n",
    "        observations=observations,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        next_observations=next_observations,\n",
    "        terminals=terminals,\n",
    "        success=success,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41596f45",
   "metadata": {},
   "source": [
    "```python\n",
    "net = hgnn()\n",
    "pol = epsilonGreedyPolicy(net, 0.)\n",
    "\n",
    "paths = [sample_episode(env, pol, 15) for _ in range(10)]\n",
    "env.render()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8f130c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "\n",
    "grep = lambda q, x : list(map(q.__getitem__, x))\n",
    "grepslice = lambda q, x1, x2 : list(itertools.islice(q, x1, x2))\n",
    "to_batch = lambda q : torch.stack(list(q))\n",
    "softmax = lambda x : np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "Sample = namedtuple(\"Sample\", \"s a r sp d\")\n",
    "\n",
    "class replayBuffer():\n",
    "    def __init__(self, max_replay_buffer_size, replace = True, prioritized=False):\n",
    "        self._max_replay_buffer_size = max_replay_buffer_size        \n",
    "        self._replace = replace\n",
    "        self._prioritized = prioritized\n",
    "        \n",
    "        self._weights = deque([], max_replay_buffer_size)\n",
    "        self._samples = deque([], max_replay_buffer_size)            \n",
    "    \n",
    "    def add_paths(self, paths):\n",
    "        for path in paths:\n",
    "            self.add_path_samples(path)\n",
    "    \n",
    "    def add_path_samples(self, path):\n",
    "        for s, a, r, sp, d, w in zip(path[\"observations\"],\n",
    "                                     path[\"actions\"],\n",
    "                                     path[\"rewards\"],\n",
    "                                     path[\"next_observations\"],\n",
    "                                     path[\"terminals\"],\n",
    "                                     path[\"rewards\"]):\n",
    "            self._samples.appendleft(Sample(s, a, r, sp, d))\n",
    "            self._weights.appendleft(w)\n",
    "        \n",
    "        self.terminate_episode()\n",
    "        \n",
    "    def terminate_episode(self):\n",
    "        pass\n",
    "\n",
    "    def random_batch(self, batch_size):\n",
    "        prio = softmax(self._weights) if self._prioritized else None\n",
    "        indices = np.random.choice(self.get_size(), \n",
    "                                   size=batch_size, p=prio, \n",
    "                                   replace=self._replace or self._size < batch_size)\n",
    "        if not self._replace and self._size < batch_size:\n",
    "            warn('Replace was set to false, but is temporarily set to true \\\n",
    "            because batch size is larger than current size of replay.')\n",
    "        \n",
    "        return grep(self._samples, indices)\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self._weights)\n",
    "        \n",
    "    def num_steps_can_sample(self):\n",
    "        return self._max_replay_buffer_size - self.get_size()\n",
    "\n",
    "    def get_diagnostics(self):\n",
    "        return OrderedDict([\n",
    "            ('size', self.get_size())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eace7b1",
   "metadata": {},
   "source": [
    "```python\n",
    "replay_buffer_cap = 4000 #10000\n",
    "replay_buffer = replayBuffer(replay_buffer_cap, prioritized=True)\n",
    "\n",
    "replay_buffer.add_paths(paths)\n",
    "batch = replay_buffer.random_batch(6)\n",
    "\n",
    "batched_g = batch_graphs([b.s for b in batch])\n",
    "\n",
    "qf = hgnn()\n",
    "out = qf(batched_g)\n",
    "out.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2647a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(g, scores):\n",
    "    n = scores.shape[0]\n",
    "    idx = (g.ndata['hv']['job'][:, 3] == 0).view(n, -1)\n",
    "    \n",
    "    values, workers = scores.max(-1, keepdims=False)\n",
    "    return torch.stack([values[i][idx[i]].max() if sum(idx[i]).item()>0 else torch.tensor(0.) for i in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d018e8d1",
   "metadata": {},
   "source": [
    "```python\n",
    "node_ids = batched_g.ndata['id']\n",
    "torch.cat([batched_g.nodes('worker')[node_ids['worker']==i].repeat(c)\n",
    "           for i, c in zip(*node_ids['job'].unique(return_counts=True))])\n",
    "torch.cat([batched_g.nodes('job')[node_ids['job']==i].repeat_interleave(c)\n",
    "           for i, c in zip(*node_ids['worker'].unique(return_counts=True))])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca5c20e",
   "metadata": {},
   "source": [
    "```python \n",
    "from dgl.dataloading import GraphDataLoader, DataLoader, NeighborSampler\n",
    "\n",
    "dataloader = dgl.dataloading.GraphDataLoader([b.s for b in batch],\n",
    "                                             use_ddp=False,\n",
    "                                             batch_size=2, shuffle=False,\n",
    "                                             drop_last=True, num_workers=1)\n",
    "\n",
    "for batched_graph, labels in dataloader:\n",
    "    train_on(batched_graph, labels)\n",
    "    \n",
    "# OR\n",
    "dgl.batch(batch) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b305eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'job': 2, 'worker': 2},\n",
       "      num_edges={('job', 'next', 'job'): 0, ('job', 'precede', 'job'): 1, ('worker', 'processing', 'job'): 0},\n",
       "      metagraph=[('job', 'job', 'next'), ('job', 'job', 'precede'), ('worker', 'job', 'processing')])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "njobs, nworkers = 2, 2\n",
    "env = jobShopScheduling(njobs, nworkers)\n",
    "g0 = env.reset()\n",
    "g0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4582d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = hgnn()\n",
    "expl_policy = epsilonGreedyPolicy(qf, .1)\n",
    "\n",
    "target_qf = hgnn()\n",
    "eval_policy = epsilonGreedyPolicy(target_qf, 0.)\n",
    "\n",
    "expl_path_collector = MdpPathCollector(env, expl_policy, rollout_fn=sample_episode, parallelize=False)\n",
    "eval_path_collector = MdpPathCollector(env, eval_policy, rollout_fn=sample_episode, parallelize=False)\n",
    "\n",
    "replay_buffer_cap = 512 #2000 #10000\n",
    "replay_buffer = replayBuffer(replay_buffer_cap, prioritized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa07b5",
   "metadata": {},
   "source": [
    "```python\n",
    "path = rollout(env, expl_policy, 2500)\n",
    "path['terminals'][-1]\n",
    "env.render()\n",
    "\n",
    "replay_buffer.add_path(path, env.g)\n",
    "\n",
    "replay_buffer.random_batch(50)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e59a8542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7161b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(qf.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "qf_criterion = nn.MSELoss()\n",
    "\n",
    "max_len = njobs+1\n",
    "n_samples = 128 \n",
    "n_epoch = 40\n",
    "n_iter = 64\n",
    "batch_size = 32\n",
    "gamma = 1.0\n",
    "\n",
    "loss = []\n",
    "avg_r_train = []\n",
    "avg_r_test = []\n",
    "max_r_test = []\n",
    "success_rate = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9145233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reward(paths):\n",
    "    return torch.tensor([p['rewards'] for p in paths]).sum(1).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b34da3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victorialena/anaconda3/envs/rlpyt/lib/python3.7/site-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
      "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  -> loss: 0.33286874 | rewards: (train) 0.91992199\n",
      "epoch 2  -> loss: 0.31912304 | rewards: (train) 0.85851568\n",
      "epoch 3  -> loss: 0.25576831 | rewards: (train) 0.8725\n",
      "epoch 4  -> loss: 0.238218 | rewards: (train) 1.02109385\n",
      "epoch 5  -> loss: 0.21976038 | rewards: (train) 1.06453133\n",
      "epoch 6  -> loss: 0.17723149 | rewards: (train) 1.17273438\n",
      "epoch 7  -> loss: 0.16449222 | rewards: (train) 1.10851574\n",
      "epoch 8  -> loss: 0.12188504 | rewards: (train) 1.2265625\n",
      "epoch 9  -> loss: 0.12317062 | rewards: (train) 1.20140624\n",
      "epoch 10  -> loss: 0.12417184 | rewards: (train) 1.363047\n",
      "epoch 11  -> loss: 0.14166653 | rewards: (train) 1.08929682\n",
      "epoch 12  -> loss: 0.15630536 | rewards: (train) 1.28945327\n",
      "epoch 13  -> loss: 0.12850748 | rewards: (train) 1.24164057\n",
      "epoch 14  -> loss: 0.10830698 | rewards: (train) 1.2898438\n",
      "epoch 15  -> loss: 0.12125908 | rewards: (train) 1.13851571\n",
      "epoch 16  -> loss: 0.14000158 | rewards: (train) 1.13031256\n",
      "epoch 17  -> loss: 0.12796983 | rewards: (train) 1.24585938\n",
      "epoch 18  -> loss: 0.11194516 | rewards: (train) 1.2805469\n",
      "epoch 19  -> loss: 0.11218913 | rewards: (train) 1.06882811\n",
      "epoch 20  -> loss: 0.11793118 | rewards: (train) 1.20406258\n",
      "epoch 21  -> loss: 0.10983045 | rewards: (train) 1.21367204\n",
      "epoch 22  -> loss: 0.11635034 | rewards: (train) 1.24781251\n",
      "epoch 23  -> loss: 0.10845745 | rewards: (train) 1.15812504\n",
      "epoch 24  -> loss: 0.09873421 | rewards: (train) 1.3801564\n",
      "epoch 25  -> loss: 0.0955482 | rewards: (train) 1.27953136\n",
      "epoch 26  -> loss: 0.08856028 | rewards: (train) 1.43031263\n",
      "epoch 27  -> loss: 0.07882669 | rewards: (train) 1.48789072\n",
      "epoch 28  -> loss: 0.07677404 | rewards: (train) 1.41125011\n",
      "epoch 29  -> loss: 0.08378602 | rewards: (train) 1.45945311\n",
      "epoch 30  -> loss: 0.07103184 | rewards: (train) 1.59429693\n",
      "epoch 31  -> loss: 0.06692314 | rewards: (train) 1.61242175\n",
      "epoch 32  -> loss: 0.07409061 | rewards: (train) 1.56828141\n",
      "epoch 33  -> loss: 0.05384625 | rewards: (train) 1.7464844\n",
      "epoch 34  -> loss: 0.04435623 | rewards: (train) 1.66890621\n",
      "epoch 35  -> loss: 0.05067689 | rewards: (train) 1.7196095\n",
      "epoch 36  -> loss: 0.04767474 | rewards: (train) 1.68835962\n",
      "epoch 37  -> loss: 0.04229191 | rewards: (train) 1.71789062\n",
      "epoch 38  -> loss: 0.03793588 | rewards: (train) 1.74046886\n",
      "epoch 39  -> loss: 0.04374018 | rewards: (train) 1.78781247\n",
      "epoch 40  -> loss: 0.02245381 | rewards: (train) 1.84085953\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_epoch):\n",
    "    qf.train(False)\n",
    "    paths = expl_path_collector.collect_new_paths(n_samples, max_len, False)\n",
    "    train_r = mean_reward(paths)\n",
    "    replay_buffer.add_paths(paths)\n",
    "\n",
    "    qf.train(True)\n",
    "    for _ in range(n_iter):\n",
    "        batch = replay_buffer.random_batch(batch_size)\n",
    "\n",
    "        rewards = torch.tensor([b.r for b in batch])\n",
    "        terminals =torch.tensor([b.d for b in batch]).float()\n",
    "        actions = torch.tensor([b.a for b in batch])\n",
    "        \n",
    "        states = batch_graphs([b.s for b in batch])\n",
    "        next_s = batch_graphs([b.sp for b in batch])        \n",
    "\n",
    "        out = target_qf(next_s) # shape = (|G|, |J|, |W|)\n",
    "        target_q_values = get_scores(next_s, out)\n",
    "        y_target = rewards + (1. - terminals) * gamma * target_q_values \n",
    "        \n",
    "        out = qf(states)\n",
    "        y_pred = out[torch.arange(batch_size), actions.T[1], actions.T[0]]\n",
    "        qf_loss = qf_criterion(y_pred, y_target).to(torch.float)\n",
    "\n",
    "        loss.append(qf_loss.item())\n",
    "        avg_r_train.append(rewards.mean().item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    target_qf.load_state_dict(deepcopy(qf.state_dict()))\n",
    "    err = 8\n",
    "    print(\"epoch\", i+1, #\"| lr:\", scientific_notation(optimizer.param_groups[0][\"lr\"]) ,\n",
    "          \" -> loss:\", round(np.mean(loss[-n_iter:]), err),\n",
    "          \"| rewards: (train)\", round(train_r, err))\n",
    "#           \"| (test)\", round(avg_r_test[-1], err), \"| (max)\", round(max_r_test[-1], err),\n",
    "#           \"| success rate:\", round(success_rate[-1], err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "002a225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = env.reset()\n",
    "qf.eval()\n",
    "qf(g1)\n",
    "\n",
    "paths = expl_path_collector.collect_new_paths(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3a95f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.01, 1.8600000047683716],\n",
       " [-0.01, 1.9099999570846558],\n",
       " [-0.01, 1.9200000667572021],\n",
       " [-0.01, 1.930000057220459],\n",
       " [-0.01, 1.9500000381469726],\n",
       " [-0.01, 1.8900000953674316],\n",
       " [-0.01, 1.9200000667572021],\n",
       " [-0.01, -0.01],\n",
       " [-0.01, -0.01],\n",
       " [-0.01, 1.9100000762939453]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p['rewards'] for p in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f646a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlpyt] *",
   "language": "python",
   "name": "conda-env-rlpyt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
