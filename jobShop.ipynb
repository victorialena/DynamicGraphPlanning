{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7631a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78703cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b66a53f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import MultiDiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86161490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy, deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7589178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victorialena/anaconda3/envs/rlpyt/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/home/victorialena/anaconda3/envs/rlpyt/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Sized\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "218b144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_queues(n: int, beta: float = 0.8):\n",
    "    free = [True]*n\n",
    "    out = []\n",
    "    for i in np.random.permutation(n):\n",
    "        if not free[i]:\n",
    "            continue\n",
    "        free[i] = False\n",
    "        \n",
    "        while np.random.rand() < beta:\n",
    "            try:\n",
    "                j = np.random.choice(np.where(free)[0])\n",
    "            except:\n",
    "                return out\n",
    "            free[j] = False\n",
    "            out.append([i, j])\n",
    "            i = j\n",
    "    return out\n",
    "\n",
    "def count_q_length(_from, _to, n):\n",
    "    counts, prev_counts = torch.zeros(n), torch.zeros(n)\n",
    "    counts[_from] = 1\n",
    "    while not all(counts == prev_counts):\n",
    "        prev_counts = deepcopy(counts)\n",
    "        counts[_from] = counts[_to]+1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7081aa82",
   "metadata": {},
   "source": [
    "```python\n",
    "g.nodes('job')\n",
    "g.ntypes\n",
    "g.etypes\n",
    "g.canonical_etypes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409aa48",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd55291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class jobShopScheduling(gym.Env):\n",
    "    \"\"\"\n",
    "    ### Description\n",
    "    Learning to schedule a successful sequence of “job” to multiple workers respecting given constraints. \n",
    "    \n",
    "    ### Action Space\n",
    "    By adding an edge from a worker to an unscheduled job, the job gets queued to that thread.\n",
    "    The resulting sequence can not be chnaged in hindsight.\n",
    "    \n",
    "    ### State Space    \n",
    "    A disjunctive heterogeneous graph g = (V, C U D). Each node represents a “job” or a “worker”. \n",
    "    Edges in C denote succession requirements for jobs, edges in D denotes which jobs were assigned to \n",
    "    which worker. \n",
    "    \n",
    "    ### Rewards\n",
    "    The system recieves a positive unit reward for each executed job. And a penalty per time step.\n",
    "    \n",
    "    ### Starting State\n",
    "    A random set of n jobs, including time requirements and succession constraints, e.g., task i requires \n",
    "    completion of task j.\n",
    "    \n",
    "    ### Episode Termination\n",
    "    The episode terminates when all jobs have been scheduled. Then the action space has schunken to size 0.\n",
    "    The final reward tallies up the remaining rewards to be versed (w/o time discounting).\n",
    "    \n",
    "    ### Arguments\n",
    "    No additional arguments are currently supported.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, njobs: int, nworkers: int):\n",
    "        self._njobs = njobs\n",
    "        self._nworkers = nworkers\n",
    "        self._jfeat = 7\n",
    "        self._wfeat = 3\n",
    "        self._dt = 0.1\n",
    "        self._time_penalty = -0.01\n",
    "        \n",
    "        self._state = None\n",
    "        \n",
    "    def reward(self, a):\n",
    "        assert False, \"Not implemented. Do not call.\"\n",
    "    \n",
    "    def terminal(self):\n",
    "        # Terminal state is reached when all the jobs have been scheduled. |A| is zero.\n",
    "        return all(self._state.nodes['job'].data['hv'][:, 3] == 1)\n",
    "    \n",
    "    def worker_features(self):\n",
    "        return ('n queued', 'expected run time', 'efficiency rate')\n",
    "    \n",
    "    def job_features(self):\n",
    "        return ('time req', \n",
    "                'completion%', #1\n",
    "                'nr of child nodes', #2 \n",
    "                'status (one hot: scheduled, processing, finished)', #3-4-5\n",
    "                'remaining time') #6\n",
    "    \n",
    "    def valid_action(self, a):\n",
    "        _, j = a\n",
    "        return self._state.nodes['job'].data['hv'][j, 3] == 0\n",
    "    \n",
    "    def check_job_requirements(self, j):\n",
    "        # Return True if no incoming edges from preceding job requirements.\n",
    "        _, dst = self._state.edges(etype='precede')\n",
    "        return all(dst != j)\n",
    "    \n",
    "    def rollout(self, verbose=False):\n",
    "        # Return number of jobs complete if we just waited until all workers exit (done of gridlock)\n",
    "        # Does not take into account discount factor!\n",
    "        state_hv = deepcopy(self._state.nodes['job'].data['hv'])\n",
    "        state_he = deepcopy(self._state.nodes['worker'].data['he'])\n",
    "        \n",
    "        jdone = state_hv[:, 5] == 1\n",
    "        \n",
    "        reward = torch.tensor([0.])\n",
    "        src, dst = deepcopy(self._state.edges(etype='processing'))\n",
    "        sreq, dreq = deepcopy(self._state.edges(etype='precede'))\n",
    "        \n",
    "        while True:\n",
    "            idx = [dst[src==w][0].item() for w in src.unique().tolist()]\n",
    "            idx = [j for j in idx if all(jdone[sreq[dreq==j]])]\n",
    "            if len(idx) == 0:\n",
    "                break # gridlock\n",
    "            \n",
    "            # get smallest remaining time for idx. -(.dt)\n",
    "            j = idx[state_hv[idx, 6].argmin().item()]\n",
    "            if verbose:\n",
    "                print(\"executing job\", j, \"on worker\", src[dst==j].item())\n",
    "            jdone[j] = True\n",
    "            reward += 1. + state_hv[j, 6].div(self._dt, rounding_mode='trunc')*self._time_penalty\n",
    "            state_hv[idx, 6] -= state_hv[j, 6] #mark that job as done\n",
    "            \n",
    "            # remove job from queue\n",
    "            src = src[dst!=j]\n",
    "            dst = dst[dst!=j]\n",
    "            \n",
    "        # clean up graph\n",
    "        idx = np.where(jdone)[0]\n",
    "        \n",
    "        src, dst, cnts = self._state.edges('all', etype='processing')\n",
    "        jidx = [(j in idx) for j in dst]\n",
    "        self._state.remove_edges(cnts[jidx].tolist(), 'processing')                \n",
    "        \n",
    "        for etype in ['next', 'precede']:\n",
    "            src, dst, cnts = self._state.edges('all', etype=etype)\n",
    "            jidx = [(j in idx) for j in src]\n",
    "            self._state.remove_edges(cnts[jidx].tolist(), etype)\n",
    "            \n",
    "        self._state.nodes['job'].data['hv'][idx, 2] = 1.\n",
    "        self._state.nodes['job'].data['hv'][idx, 4] = 0.\n",
    "        self._state.nodes['job'].data['hv'][idx, 5] = 1. # mark terminal\n",
    "        self._state.nodes['job'].data['hv'][idx, 6] = 0. # set remaining time to 0\n",
    "        \n",
    "        src, _ = self._state.edges(etype='processing')\n",
    "        if len(src):\n",
    "            w, cnts = src.unique(return_counts=True)\n",
    "            self._state.nodes['worker'].data['he'][w, 0] = cnts.float()\n",
    "\n",
    "        return reward.item(), all(jdone)\n",
    "    \n",
    "    def get_node_status(self, label, by_index=True):\n",
    "        mask = self._state.nodes['job'].data['hv'][:, label] == 1\n",
    "        if by_index:\n",
    "            return np.where(mask)[0].tolist()\n",
    "        return mask\n",
    "        \n",
    "    def get_scheduled(self, by_index=True):\n",
    "        return self.get_node_status(3, by_index)\n",
    "    \n",
    "    def get_unscheduled(self, by_index=True):\n",
    "        mask = ~self.get_node_status(3, False)\n",
    "        if by_index:\n",
    "            return np.where(mask)[0].tolist()\n",
    "        return mask\n",
    "    \n",
    "    def get_processing(self, by_index=True):\n",
    "        return self.get_node_status(4, by_index)\n",
    "    \n",
    "    def get_terminated(self, by_index=True):\n",
    "        return self.get_node_status(5, by_index)\n",
    "    \n",
    "    def is_gridlocked(self):\n",
    "        if len(self.get_processing()):\n",
    "            return False\n",
    "\n",
    "        src, dst = self._state.edges(etype='processing')\n",
    "        if len(src.unique()) != self._nworkers:\n",
    "            return False\n",
    "\n",
    "        _, req = self._state.edges(etype='precede')\n",
    "        newidx = [dst[src==w][0].item() for w in src.unique().tolist()]\n",
    "        newidx = [j for j in newidx if j not in req]\n",
    "        return len(newidx)>0\n",
    "    \n",
    "    def step(self, a):\n",
    "        assert self.valid_action(a), \"Invalid action taken: (w:%d, j:%d)\" % a\n",
    "        \n",
    "        src, dst, cnts = self._state.edges('all', etype='processing')\n",
    "        \n",
    "        \"\"\" \n",
    "        1) Schedule job j for worker w: \n",
    "            a) Find last job scheduled for worker w, add edge from end of queue to new job j. \n",
    "            b) Add edge from w to j. \n",
    "            c) Update worker info (queue length, run time estimate).\n",
    "            d) Mark job as scheduled.\n",
    "        \"\"\"\n",
    "        w, j = a\n",
    "        if w in src:\n",
    "            _i = dst[src==w][-1].item() # add to end of q -- last edge added\n",
    "            self._state.add_edge(_i, j, etype='next')        \n",
    "        self._state.add_edge(w, j, etype='processing')\n",
    "        \n",
    "        state_hv = deepcopy(self._state.nodes['job'].data['hv'])\n",
    "        state_he = deepcopy(self._state.nodes['worker'].data['he'])\n",
    "        \n",
    "        state_he[w, 0] += 1. # add job to work queue length\n",
    "        state_he[w, 1] += state_hv[j, 0] # update worker' run time estimate\n",
    "        state_hv[j, 3] = 1. # mark as scheduled\n",
    "                \n",
    "        \"\"\" 2) Assure the first job in queue is being processed at this time step. \"\"\"\n",
    "        _, req = self._state.edges(etype='precede')\n",
    "        # src, dst, cnts = self._state.edges('all', etype='processing') # call again to update processing\n",
    "        newidx = [dst[src==w][0].item() for w in src.unique().tolist()]\n",
    "        newidx = [j for j in newidx if j not in req]\n",
    "        state_hv[newidx, 4] = 1 # set to processing (but completion % remain 0)\n",
    "        \n",
    "        # write info incase of early exit\n",
    "        self._state.nodes['job'].data['hv'] = state_hv\n",
    "        self._state.nodes['worker'].data['he'] = state_he\n",
    "\n",
    "        \"\"\" \n",
    "        3) Update feature vectors:\n",
    "            a) Progress time for node features: remaining time, completion % for jobs and workers\n",
    "            b) Update info around terminal jobs, and remove processing edge if job has terminated.\n",
    "            c) Remove next and precede edges for terminated jobs. \n",
    "        \"\"\"\n",
    "        # a\n",
    "        processing_mask = state_hv[:, 4] == 1        \n",
    "        if processing_mask.sum() == 0:\n",
    "            return deepcopy(self._state), self._time_penalty, self.terminal(), {'success':False}\n",
    "        \n",
    "        state_hv[processing_mask, 6] = torch.maximum(state_hv[processing_mask, 6]-self._dt,\n",
    "                                                     torch.zeros(processing_mask.sum())).round(decimals=2) # update remaining time\n",
    "        state_hv[processing_mask, 1] = torch.clamp(1-torch.div(state_hv[processing_mask, 6],\n",
    "                                                               state_hv[processing_mask, 0]), \n",
    "                                                   min=0, max=1) # update completion %\n",
    "        \n",
    "        state_he[:, 1] = torch.maximum(state_he[:, 1]-self._dt, torch.zeros(self._nworkers)) # update remaining time\n",
    "        \n",
    "        # b\n",
    "        state_hv[processing_mask, 5] = (state_hv[processing_mask, 1] == 1).float() # mark terminal\n",
    "        state_hv[processing_mask, 4] = 1-state_hv[processing_mask, 5] # if terminal, job no longer processing        \n",
    "        idx = torch.where(processing_mask)[0][torch.where(state_hv[processing_mask, 5])[0]].tolist() # job ids just terminated\n",
    "        if len(idx):\n",
    "            widx = [(j in idx) for j in dst]\n",
    "            state_he[src[widx], 0] -= 1 # remove job from job count\n",
    "            self._state.remove_edges(cnts[widx].tolist(), 'processing') # delete those edges?\n",
    "        \n",
    "            # c\n",
    "            src, dst, cnts = self._state.edges('all', etype='next')\n",
    "            ptridx = torch.tensor([cnts[src == j].item() for j in idx if j in src]) # this works because it is a queue: unique next node\n",
    "            if len(ptridx):\n",
    "                self._state.remove_edges(ptridx.tolist(), 'next')\n",
    "\n",
    "            src, dst, cnts = self._state.edges('all', etype='precede')\n",
    "            jidx = [(j in idx) for j in src]\n",
    "            if len(jidx):\n",
    "                self._state.remove_edges(cnts[jidx].tolist(), 'precede') # delete those edges?\n",
    "                \n",
    "        \"\"\" 5) Update feature vectors. \"\"\"\n",
    "        self._state.nodes['job'].data['hv'] = state_hv\n",
    "        self._state.nodes['worker'].data['he'] = state_he\n",
    "                \n",
    "        \"\"\" 6) Compute reward and terminal state. \"\"\"\n",
    "        done = self.terminal()\n",
    "        success = False\n",
    "        n_terminal = len(idx)\n",
    "        reward = self._time_penalty + n_terminal\n",
    "        \n",
    "        if done:\n",
    "            r, success = self.rollout()\n",
    "            reward += r\n",
    "        \n",
    "        return deepcopy(self._state), deepcopy(reward), deepcopy(done), {'success':success}\n",
    "\n",
    "    def reset(self, seed: int = None, topology: str = 'random'):\n",
    "        if not seed == None:\n",
    "            super().reset(seed=seed)\n",
    "        \n",
    "        nw, nj = self._nworkers, self._njobs\n",
    "        _from, _to = torch.tensor([[0, 0]]+get_random_queues(nj)).T\n",
    "        \n",
    "        graph_data = {\n",
    "           ('job', 'precede', 'job'): (_from, _to), # A ---before---> B\n",
    "           ('job', 'next', 'job'): (torch.tensor([0]), torch.tensor([0])), # jobshop queue\n",
    "           ('worker', 'processing', 'job'): (torch.tensor([0]), torch.tensor([0])) # nothing is scheduled\n",
    "        }\n",
    "        \n",
    "        self._state = dgl.heterograph(graph_data, num_nodes_dict={'worker': nw, 'job': nj})\n",
    "        # hack: can not init null vector for edges\n",
    "        self._state.remove_edges(0, 'processing')\n",
    "        self._state.remove_edges(0, 'precede')\n",
    "        self._state.remove_edges(0, 'next')\n",
    "                \n",
    "        times = 0.1*torch.randint(1, 10, (nj,1)) # torch.rand(nj,1)\n",
    "        _from, _to = _from[1:], _to[1:]\n",
    "        counts = count_q_length(_from, _to, nj).unsqueeze(-1)\n",
    "        self._state.nodes['job'].data['hv'] = torch.cat((times, torch.zeros(nj, 1), counts, torch.zeros(nj, 3), times), 1)\n",
    "        self._state.nodes['worker'].data['he'] = torch.cat((torch.zeros(nw,2), torch.ones(nw,1)), 1)\n",
    "        \n",
    "        return deepcopy(self._state)\n",
    "    \n",
    "    def dump_state_info(self):\n",
    "        print('scheduled:', env.get_scheduled())\n",
    "        print('processing:', env.get_processing())\n",
    "        print('terminated:', env.get_terminated())\n",
    "        print('job data:')\n",
    "        print(self._state.nodes['job'].data['hv'])\n",
    "        print('worker data:')\n",
    "        print(self._state.nodes['worker'].data['he'])\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        import networkx as nx\n",
    "        import matplotlib.pyplot as plt\n",
    "        \"\"\"\n",
    "        self.dump_state_info()\n",
    "        \n",
    "        G = dgl.to_homogeneous(self._state).to_networkx(edge_attrs=['_TYPE'])\n",
    "        \n",
    "        node_color = ['red']*self._njobs+['orange']*self._nworkers\n",
    "        edge_color = ['red' if e[-1].item() % 2 else 'orange' for e in G.edges(data='_TYPE')]\n",
    "        nx.draw(G, node_color=node_color, edge_color=edge_color, with_labels=True)\n",
    "        \n",
    "    def seed(self, n: int):\n",
    "        super().reset(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da001a",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0645264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "import dgl.nn.pytorch as dglnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dbaa49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_readout_graph(g, etype):\n",
    "    \"\"\" \n",
    "    Returns graph of edges to be evalutated, i.e., draw edges from each worker to each job node.\n",
    "    If g is a batched graph representation, return edges within each subgraph.\n",
    "    etype = use .canonical_etypes() \n",
    "    \"\"\"\n",
    "    \n",
    "    utype, _, vtype = etype\n",
    "    nu, nv = g.num_nodes(utype), g.num_nodes(vtype)\n",
    "    if not is_batched_graph(g):\n",
    "        src, dst = g.nodes(utype).repeat(nv), g.nodes(vtype).repeat_interleave(nu)\n",
    "        \n",
    "        return dgl.heterograph({etype: (src, dst)},\n",
    "                               num_nodes_dict={utype: nu, vtype: nv})\n",
    "    # else:\n",
    "    node_ids = g.ndata['id']\n",
    "    src = torch.cat([g.nodes(utype)[node_ids[utype]==i].repeat(c)\n",
    "                     for i, c in zip(*node_ids[vtype].unique(return_counts=True))])\n",
    "    dst = torch.cat([g.nodes(vtype)[node_ids[vtype]==i].repeat_interleave(c)\n",
    "                     for i, c in zip(*node_ids[utype].unique(return_counts=True))])\n",
    "    \n",
    "    out = dgl.heterograph({etype: (src, dst)}, num_nodes_dict={utype: nu, vtype: nv})\n",
    "    out.nodes['job'].data['id'] = g.nodes['job'].data['id']\n",
    "    out.nodes['worker'].data['id'] = g.nodes['worker'].data['id']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f4cce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_batched_graph(g):\n",
    "    return len(g.ndata['id'])!=0\n",
    "\n",
    "def num_subgraphs(g):\n",
    "    return 1 if not is_batched_graph(g) else len(g.ndata['id']['job'].unique())\n",
    "\n",
    "def sg_nworkers(g):\n",
    "    if is_batched_graph(g):\n",
    "        idx, cnts = g.ndata['id']['worker'].unique(return_counts=True)\n",
    "        if len(cnts.unique()) == 1:\n",
    "            return cnts.unique().item()\n",
    "        return cnts\n",
    "    return g.num_nodes('worker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efde7d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotProductPredictor(nn.Module):\n",
    "    \"\"\" returns scores for each job (row) per worker (col)\"\"\"\n",
    "    def forward(self, graph, hv, he, _etype):\n",
    "        # hv contains the node representations computed from the GNN\n",
    "        utype, etype, vtype = _etype\n",
    "        nu = sg_nworkers(graph)\n",
    "        assert type(nu)==int, \"Graphs have different workers counts. Can not proceed this eval without errors.\"\n",
    "        with graph.local_scope():\n",
    "            graph.nodes[vtype].data['hv'] = hv\n",
    "            graph.nodes[utype].data['he'] = he\n",
    "            graph.apply_edges(fn.u_dot_v('he', 'hv', 'score'), etype=etype)\n",
    "            return graph.edges[etype].data['score'].view(num_subgraphs(graph), -1, nu).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21de416e",
   "metadata": {},
   "source": [
    "```python\n",
    "class agent(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, rel_names):\n",
    "        super().__init__()\n",
    "        self.sage = RGCN(in_features, hidden_features, out_features, rel_names)\n",
    "        self.pred = dotProductPredictor()\n",
    "    \n",
    "    def forward(self, g, x, etype):        \n",
    "        hv = self.sage(g, x)\n",
    "        he = g.nodes['worker'].data['he']\n",
    "        rg = construct_readout_graph(g, ('worker', 'processing', 'job'))\n",
    "        return self.pred(rg, hv, he, ('worker', 'processing', 'job'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56099912",
   "metadata": {},
   "source": [
    "```python\n",
    "from dgl.nn.pytorch.conv import HGTConv\n",
    "layer = HGTConv(7, head_size=16, num_heads=4, num_ntypes=2, num_etypes=3, dropout=0.1, use_norm=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d170b94",
   "metadata": {},
   "source": [
    "```python\n",
    "env = jobShopScheduling(10, 2)\n",
    "g = env.reset()\n",
    "env.render()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2979510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hgnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = dglnn.HeteroLinear({'job': 7, 'worker':3}, 16)\n",
    "        self.conv = dglnn.HeteroGraphConv({\n",
    "            'precede' : dglnn.GraphConv(16, 16),\n",
    "            'next' : dglnn.GraphConv(16, 16),\n",
    "            'processing' : dglnn.SAGEConv((16, 16), 16, 'mean')},\n",
    "            aggregate='sum')\n",
    "        self.pred = dotProductPredictor()\n",
    "        \n",
    "    def forward(self, g):\n",
    "        h0 = {**g.ndata['hv'], **g.ndata['he']}\n",
    "        h1 = self.embedding(h0)\n",
    "        hv = self.conv(g, h1)['job']\n",
    "        \n",
    "        rg = construct_readout_graph(g, ('worker', 'processing', 'job'))\n",
    "        return self.pred(rg, hv, h1['worker'], ('worker', 'processing', 'job'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "807d5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_graphs(batch):\n",
    "    n = len(batch)\n",
    "    \n",
    "    njs = torch.tensor([g.num_nodes('job') for g in batch])\n",
    "    nws = torch.tensor([g.num_nodes('worker') for g in batch])\n",
    "    \n",
    "    def concat_edges(etype, idx, ns):\n",
    "        return torch.cat([batch[i].edges(etype=etype)[idx]+ns[:i].sum() for i in range(n)])\n",
    "    \n",
    "    batched_graph_data = {\n",
    "        ('job', 'precede', 'job'): (concat_edges('precede', 0, njs), concat_edges('precede', 1, njs)), \n",
    "        ('job', 'next', 'job'): (concat_edges('next', 0, njs), concat_edges('next', 1, njs)),\n",
    "        ('worker', 'processing', 'job'): (concat_edges('processing', 0, nws), concat_edges('processing', 1, njs)),\n",
    "    }\n",
    "\n",
    "    state = dgl.heterograph(batched_graph_data, \n",
    "                            num_nodes_dict={'worker': nws.sum().item(), 'job': njs.sum().item()})\n",
    "\n",
    "    state.nodes['job'].data['hv'] = torch.cat([g.nodes['job'].data['hv'] for g in batch])\n",
    "    state.nodes['worker'].data['he'] = torch.cat([g.nodes['worker'].data['he'] for g in batch])\n",
    "    state.nodes['job'].data['id'] = torch.arange(n).repeat_interleave(njs)\n",
    "    state.nodes['worker'].data['id'] = torch.arange(n).repeat_interleave(nws)\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec2e223",
   "metadata": {},
   "source": [
    "### Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f50ae0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/victorialena/rlkit')\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0771f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlkit.policies.base import Policy\n",
    "from path_collector import MdpPathCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1ab6af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scientific_notation =  lambda x:\"{:.2e}\".format(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30e14faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class epsilonGreedyPolicy(nn.Module, Policy):\n",
    "    def __init__(self, net, eps=0.1):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.eps = np.clip(eps, .0, 1.)\n",
    "\n",
    "    def __call__(self, g):\n",
    "        return self.get_action(g)\n",
    "    \n",
    "    def get_action(self, g):\n",
    "        idx = np.where(g.ndata['hv']['job'][:, 3] == 0)[0] # idx: list of unscheduled job ids\n",
    "        assert len(idx) > 0, \"Unecessary query. Empty action space.\"\n",
    "        \n",
    "        if np.random.rand() < self.eps:\n",
    "            nw = g.num_nodes(ntype='worker')\n",
    "            return (np.random.randint(nw), np.random.choice(idx))\n",
    "        \n",
    "        out = self.net(g)\n",
    "        val, workers = out[idx].max(1)\n",
    "        j = val.argmax().item()\n",
    "        w = workers[j].item()\n",
    "        return (w, idx[j])\n",
    "    \n",
    "    def get_actions(self, g):\n",
    "        assert is_batched_graph(g), \"This is a single graph, call get_action() instead.\"\n",
    "        \n",
    "        n = num_subgraphs(g)\n",
    "        idx = (g.ndata['hv']['job'][:, 3] == 0).view(n, -1)\n",
    "        assert sum(idx).item() > 0, \"Empty action space!\"\n",
    "        \n",
    "        scores = self.net(g)\n",
    "        out = {}\n",
    "        values, workers = scores.max(-1, keepdims=False)\n",
    "        for i in range(n):\n",
    "            j = torch.where(idx[i])[0][values[i, idx[i]].argmax().item()].item()\n",
    "            out[i] = (workers[i, j].item(), j)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f810b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_episode(env, agent, max_path_length:int=100):\n",
    "    obs = env.reset()\n",
    "    \n",
    "    observations=[]\n",
    "    actions=[]\n",
    "    rewards=[]\n",
    "    next_observations=[]\n",
    "    terminals=[]\n",
    "    \n",
    "    success=False\n",
    "    \n",
    "    for _ in range(max_path_length):\n",
    "        a = agent.get_action(obs)\n",
    "        nextobs, reward, done, info = env.step(a)\n",
    "        \n",
    "        observations.append(obs)\n",
    "        actions.append(deepcopy(a))\n",
    "        rewards.append(reward)\n",
    "        next_observations.append(nextobs)\n",
    "        terminals.append(done)\n",
    "        \n",
    "        obs = nextobs\n",
    "        success = info['success']\n",
    "        \n",
    "        if done: \n",
    "            break\n",
    "            \n",
    "    return dict(\n",
    "        observations=observations,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        next_observations=next_observations,\n",
    "        terminals=terminals,\n",
    "        success=success,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facdf03b",
   "metadata": {},
   "source": [
    "```python\n",
    "net = hgnn()\n",
    "pol = epsilonGreedyPolicy(net, 0.)\n",
    "\n",
    "paths = [sample_episode(env, pol, 15) for _ in range(10)]\n",
    "env.render()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19fd4306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "\n",
    "grep = lambda q, x : list(map(q.__getitem__, x))\n",
    "grepslice = lambda q, x1, x2 : list(itertools.islice(q, x1, x2))\n",
    "to_batch = lambda q : torch.stack(list(q))\n",
    "softmax = lambda x : np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "Sample = namedtuple(\"Sample\", \"s a r sp d\")\n",
    "\n",
    "class replayBuffer():\n",
    "    def __init__(self, max_replay_buffer_size, replace = True, prioritized=False):\n",
    "        self._max_replay_buffer_size = max_replay_buffer_size        \n",
    "        self._replace = replace\n",
    "        self._prioritized = prioritized\n",
    "        \n",
    "        self._weights = deque([], max_replay_buffer_size)\n",
    "        self._samples = deque([], max_replay_buffer_size)            \n",
    "    \n",
    "    def add_paths(self, paths):\n",
    "        for path in paths:\n",
    "            self.add_path_samples(path)\n",
    "    \n",
    "    def add_path_samples(self, path):\n",
    "        for s, a, r, sp, d, w in zip(path[\"observations\"],\n",
    "                                     path[\"actions\"],\n",
    "                                     path[\"rewards\"],\n",
    "                                     path[\"next_observations\"],\n",
    "                                     path[\"terminals\"],\n",
    "                                     path[\"rewards\"]):\n",
    "            self._samples.appendleft(Sample(s, a, r, sp, d))\n",
    "            self._weights.appendleft(w)\n",
    "        \n",
    "        self.terminate_episode()\n",
    "        \n",
    "    def terminate_episode(self):\n",
    "        pass\n",
    "\n",
    "    def random_batch(self, batch_size):\n",
    "        prio = softmax(self._weights) if self._prioritized else None\n",
    "        indices = np.random.choice(self.get_size(), \n",
    "                                   size=batch_size, p=prio, \n",
    "                                   replace=self._replace or self._size < batch_size)\n",
    "        if not self._replace and self._size < batch_size:\n",
    "            warn('Replace was set to false, but is temporarily set to true \\\n",
    "            because batch size is larger than current size of replay.')\n",
    "        \n",
    "        return grep(self._samples, indices)\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self._weights)\n",
    "        \n",
    "    def num_steps_can_sample(self):\n",
    "        return self._max_replay_buffer_size - self.get_size()\n",
    "\n",
    "    def get_diagnostics(self):\n",
    "        return OrderedDict([\n",
    "            ('size', self.get_size())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1a9813",
   "metadata": {},
   "source": [
    "```python\n",
    "replay_buffer_cap = 4000 #10000\n",
    "replay_buffer = replayBuffer(replay_buffer_cap, prioritized=True)\n",
    "\n",
    "replay_buffer.add_paths(paths)\n",
    "batch = replay_buffer.random_batch(6)\n",
    "\n",
    "batched_g = batch_graphs([b.s for b in batch])\n",
    "\n",
    "qf = hgnn()\n",
    "out = qf(batched_g)\n",
    "out.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7943c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(g, scores):\n",
    "    n = scores.shape[0]\n",
    "    idx = (g.ndata['hv']['job'][:, 3] == 0).view(n, -1)\n",
    "    \n",
    "    values, workers = scores.max(-1, keepdims=False)\n",
    "    return torch.stack([values[i][idx[i]].max() if sum(idx[i]).item()>0 else torch.tensor(0.) for i in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774518b",
   "metadata": {},
   "source": [
    "```python\n",
    "node_ids = batched_g.ndata['id']\n",
    "torch.cat([batched_g.nodes('worker')[node_ids['worker']==i].repeat(c)\n",
    "           for i, c in zip(*node_ids['job'].unique(return_counts=True))])\n",
    "torch.cat([batched_g.nodes('job')[node_ids['job']==i].repeat_interleave(c)\n",
    "           for i, c in zip(*node_ids['worker'].unique(return_counts=True))])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0d3b4b",
   "metadata": {},
   "source": [
    "```python \n",
    "from dgl.dataloading import GraphDataLoader, DataLoader, NeighborSampler\n",
    "\n",
    "dataloader = dgl.dataloading.GraphDataLoader([b.s for b in batch],\n",
    "                                             use_ddp=False,\n",
    "                                             batch_size=2, shuffle=False,\n",
    "                                             drop_last=True, num_workers=1)\n",
    "\n",
    "for batched_graph, labels in dataloader:\n",
    "    train_on(batched_graph, labels)\n",
    "    \n",
    "# OR\n",
    "dgl.batch(batch) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af77f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ec0c1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'job': 5, 'worker': 2},\n",
       "      num_edges={('job', 'next', 'job'): 0, ('job', 'precede', 'job'): 3, ('worker', 'processing', 'job'): 0},\n",
       "      metagraph=[('job', 'job', 'next'), ('job', 'job', 'precede'), ('worker', 'job', 'processing')])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "njobs, nworkers = 5, 2\n",
    "env = jobShopScheduling(njobs, nworkers)\n",
    "g0 = env.reset()\n",
    "g0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "deebf195",
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = hgnn()\n",
    "expl_policy = epsilonGreedyPolicy(qf, .1)\n",
    "\n",
    "target_qf = hgnn()\n",
    "eval_policy = epsilonGreedyPolicy(target_qf, 0.)\n",
    "\n",
    "expl_path_collector = MdpPathCollector(env, expl_policy, rollout_fn=sample_episode, parallelize=False)\n",
    "eval_path_collector = MdpPathCollector(env, eval_policy, rollout_fn=sample_episode, parallelize=False)\n",
    "\n",
    "replay_buffer_cap = 2000 #10000\n",
    "replay_buffer = replayBuffer(replay_buffer_cap, prioritized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a368e",
   "metadata": {},
   "source": [
    "```python\n",
    "path = rollout(env, expl_policy, 2500)\n",
    "path['terminals'][-1]\n",
    "env.render()\n",
    "\n",
    "replay_buffer.add_path(path, env.g)\n",
    "\n",
    "replay_buffer.random_batch(50)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b5c09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5c40d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(qf.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "qf_criterion = nn.MSELoss()\n",
    "\n",
    "max_len = njobs+1\n",
    "n_samples = 128 \n",
    "n_epoch = 200\n",
    "n_iter = 64\n",
    "batch_size = 32\n",
    "gamma = 1.0\n",
    "\n",
    "loss = []\n",
    "avg_r_train = []\n",
    "avg_r_eval = []\n",
    "success_rates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45fe803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reward(paths):\n",
    "    return torch.tensor([p['rewards'] for p in paths]).sum(1).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9498fc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victorialena/anaconda3/envs/rlpyt/lib/python3.7/site-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
      "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  -> loss: 15.10689212 | rewards: (train) 0.25054684 (test) 1.49593735 | success rate: 0.1875\n",
      "epoch 2  -> loss: 7.64759813 | rewards: (train) 0.22859375 (test) 0.04 | success rate: 0.0\n",
      "epoch 3  -> loss: 5.31185205 | rewards: (train) 0.1759375 (test) 0.01031251 | success rate: 0.0\n",
      "epoch 4  -> loss: 2.94169285 | rewards: (train) 0.04750001 (test) -0.05 | success rate: 0.0\n",
      "epoch 5  -> loss: 2.33805783 | rewards: (train) 0.06124999 (test) 0.0378125 | success rate: 0.0\n",
      "epoch 6  -> loss: 1.59240168 | rewards: (train) 0.25078127 (test) -0.05 | success rate: 0.0\n",
      "epoch 7  -> loss: 1.7506359 | rewards: (train) 0.12242186 (test) -0.05 | success rate: 0.0\n",
      "epoch 8  -> loss: 2.17546178 | rewards: (train) 0.13117187 (test) -0.05 | success rate: 0.0\n",
      "epoch 9  -> loss: 1.95334206 | rewards: (train) 0.07992189 (test) -0.05 | success rate: 0.0\n",
      "epoch 10  -> loss: 1.99678652 | rewards: (train) 0.1621875 (test) 0.0390625 | success rate: 0.0\n",
      "epoch 11  -> loss: 2.42660038 | rewards: (train) 0.12343747 (test) -0.05 | success rate: 0.0\n",
      "epoch 12  -> loss: 2.30588817 | rewards: (train) 0.16093749 (test) -0.05 | success rate: 0.0\n",
      "epoch 13  -> loss: 2.62812873 | rewards: (train) 0.29539061 (test) -0.05 | success rate: 0.0\n",
      "epoch 14  -> loss: 2.07833503 | rewards: (train) 0.27320313 (test) -0.05 | success rate: 0.0\n",
      "epoch 15  -> loss: 1.88024198 | rewards: (train) 0.07828125 (test) -0.05 | success rate: 0.0\n",
      "epoch 16  -> loss: 1.9398241 | rewards: (train) 0.31718749 (test) -0.05 | success rate: 0.0\n",
      "epoch 17  -> loss: 2.19190235 | rewards: (train) 0.24890621 (test) -0.05 | success rate: 0.0\n",
      "epoch 18  -> loss: 2.27109297 | rewards: (train) 0.11554684 (test) -0.05 | success rate: 0.0\n",
      "epoch 19  -> loss: 2.34458065 | rewards: (train) 0.13148439 (test) -0.05 | success rate: 0.0\n",
      "epoch 20  -> loss: 1.86006045 | rewards: (train) 0.26585937 (test) -0.05 | success rate: 0.0\n",
      "epoch 21  -> loss: 1.45003191 | rewards: (train) 0.2525 (test) -0.05 | success rate: 0.0\n",
      "epoch 22  -> loss: 2.3019224 | rewards: (train) 0.21046869 (test) -0.02124999 | success rate: 0.0\n",
      "epoch 23  -> loss: 2.13475332 | rewards: (train) 0.27874994 (test) -0.05 | success rate: 0.0\n",
      "epoch 24  -> loss: 1.97158332 | rewards: (train) 0.30249995 (test) -0.05 | success rate: 0.0\n",
      "epoch 25  -> loss: 1.68962066 | rewards: (train) 0.20679682 (test) -0.05 | success rate: 0.0\n",
      "epoch 26  -> loss: 1.53297899 | rewards: (train) 0.18351561 (test) -0.05 | success rate: 0.0\n",
      "epoch 27  -> loss: 1.54377558 | rewards: (train) 0.2192187 (test) -0.05 | success rate: 0.0\n",
      "epoch 28  -> loss: 1.8892864 | rewards: (train) 0.36226559 (test) -0.05 | success rate: 0.0\n",
      "epoch 29  -> loss: 1.92022967 | rewards: (train) 0.18359372 (test) -0.05 | success rate: 0.0\n",
      "epoch 30  -> loss: 2.04963159 | rewards: (train) 0.22195309 (test) 0.10124996 | success rate: 0.03125\n",
      "epoch 31  -> loss: 2.55036434 | rewards: (train) 0.43554679 (test) 0.09906245 | success rate: 0.03125\n",
      "epoch 32  -> loss: 2.11854958 | rewards: (train) 0.29374996 (test) -0.05 | success rate: 0.0\n",
      "epoch 33  -> loss: 1.79479756 | rewards: (train) 0.14484373 (test) -0.05 | success rate: 0.0\n",
      "epoch 34  -> loss: 1.70409288 | rewards: (train) 0.27374995 (test) -0.05 | success rate: 0.0\n",
      "epoch 35  -> loss: 1.38261169 | rewards: (train) 0.35351554 (test) -0.05 | success rate: 0.0\n",
      "epoch 36  -> loss: 1.77081992 | rewards: (train) 0.38890621 (test) 0.30656248 | success rate: 0.0625\n",
      "epoch 37  -> loss: 1.9285211 | rewards: (train) 0.30593747 (test) 0.06625 | success rate: 0.0\n",
      "epoch 38  -> loss: 1.8153675 | rewards: (train) 0.5379687 (test) 0.185625 | success rate: 0.03125\n",
      "epoch 39  -> loss: 1.54213613 | rewards: (train) 0.8010937 (test) 0.42468747 | success rate: 0.03125\n",
      "epoch 40  -> loss: 1.22747261 | rewards: (train) 0.70640624 (test) 0.45656252 | success rate: 0.03125\n",
      "epoch 41  -> loss: 1.37975931 | rewards: (train) 0.73203117 (test) 0.39437494 | success rate: 0.0625\n",
      "epoch 42  -> loss: 1.26240679 | rewards: (train) 0.60624993 (test) 0.42031252 | success rate: 0.03125\n",
      "epoch 43  -> loss: 1.27943572 | rewards: (train) 0.88351542 (test) 0.75812489 | success rate: 0.125\n",
      "epoch 44  -> loss: 1.13660654 | rewards: (train) 1.04109359 (test) 0.94343716 | success rate: 0.15625\n",
      "epoch 45  -> loss: 0.93554831 | rewards: (train) 1.42835915 (test) 1.41593719 | success rate: 0.25\n",
      "epoch 46  -> loss: 0.79942541 | rewards: (train) 1.93976533 (test) 1.93218708 | success rate: 0.34375\n",
      "epoch 47  -> loss: 0.65143604 | rewards: (train) 1.7509371 (test) 1.80249965 | success rate: 0.25\n",
      "epoch 48  -> loss: 0.54884211 | rewards: (train) 2.03046846 (test) 1.39343727 | success rate: 0.21875\n",
      "epoch 49  -> loss: 0.50181116 | rewards: (train) 2.14515567 (test) 3.14843702 | success rate: 0.5625\n",
      "epoch 50  -> loss: 0.51227304 | rewards: (train) 2.46671844 (test) 2.49874973 | success rate: 0.375\n",
      "epoch 51  -> loss: 0.49855828 | rewards: (train) 2.20101523 (test) 2.72374964 | success rate: 0.4375\n",
      "epoch 52  -> loss: 0.43432685 | rewards: (train) 2.44343686 (test) 2.82187462 | success rate: 0.5\n",
      "epoch 53  -> loss: 0.51720629 | rewards: (train) 1.86328101 (test) 1.51749969 | success rate: 0.21875\n",
      "epoch 54  -> loss: 0.4336603 | rewards: (train) 2.77023387 (test) 2.89249969 | success rate: 0.46875\n",
      "epoch 55  -> loss: 0.45616595 | rewards: (train) 2.04812455 (test) 2.2646873 | success rate: 0.28125\n",
      "epoch 56  -> loss: 0.4310406 | rewards: (train) 2.22390604 (test) 2.48562455 | success rate: 0.4375\n",
      "epoch 57  -> loss: 0.39368976 | rewards: (train) 2.49945259 (test) 2.8637495 | success rate: 0.5\n",
      "epoch 58  -> loss: 0.39573024 | rewards: (train) 2.45328093 (test) 2.26968718 | success rate: 0.34375\n",
      "epoch 59  -> loss: 0.42499158 | rewards: (train) 2.28289032 (test) 2.47874951 | success rate: 0.375\n",
      "epoch 60  -> loss: 0.46628939 | rewards: (train) 2.40796828 (test) 2.28499985 | success rate: 0.25\n",
      "epoch 61  -> loss: 0.42047059 | rewards: (train) 2.48663998 (test) 2.87468719 | success rate: 0.4375\n",
      "epoch 62  -> loss: 0.39642472 | rewards: (train) 2.54078102 (test) 2.4990623 | success rate: 0.375\n",
      "epoch 63  -> loss: 0.37970083 | rewards: (train) 2.77015591 (test) 2.39312482 | success rate: 0.28125\n",
      "epoch 64  -> loss: 0.37481474 | rewards: (train) 2.56828094 (test) 3.1074996 | success rate: 0.5\n",
      "epoch 65  -> loss: 0.37856264 | rewards: (train) 2.40789032 (test) 2.59093738 | success rate: 0.40625\n",
      "epoch 66  -> loss: 0.42155982 | rewards: (train) 2.66617179 (test) 3.05468726 | success rate: 0.53125\n",
      "epoch 67  -> loss: 0.43405537 | rewards: (train) 2.75640583 (test) 3.17656207 | success rate: 0.53125\n",
      "epoch 68  -> loss: 0.44841569 | rewards: (train) 2.58843732 (test) 2.90031171 | success rate: 0.4375\n",
      "epoch 69  -> loss: 0.48339343 | rewards: (train) 2.86109352 (test) 3.02468705 | success rate: 0.4375\n",
      "epoch 70  -> loss: 0.46688396 | rewards: (train) 2.51117182 (test) 2.51843715 | success rate: 0.34375\n",
      "epoch 71  -> loss: 0.49866245 | rewards: (train) 2.65226531 (test) 2.75437474 | success rate: 0.40625\n",
      "epoch 72  -> loss: 0.38952603 | rewards: (train) 2.61914039 (test) 2.48937488 | success rate: 0.34375\n",
      "epoch 73  -> loss: 0.40312002 | rewards: (train) 2.62031221 (test) 2.42406225 | success rate: 0.28125\n",
      "epoch 74  -> loss: 0.37982846 | rewards: (train) 2.60656214 (test) 2.9537499 | success rate: 0.375\n",
      "epoch 75  -> loss: 0.45543255 | rewards: (train) 2.69765615 (test) 2.44687462 | success rate: 0.3125\n",
      "epoch 76  -> loss: 0.43373392 | rewards: (train) 2.44687462 (test) 2.21093726 | success rate: 0.25\n",
      "epoch 77  -> loss: 0.45377343 | rewards: (train) 2.70999956 (test) 2.70249963 | success rate: 0.3125\n",
      "epoch 78  -> loss: 0.38346729 | rewards: (train) 2.64773393 (test) 2.7412498 | success rate: 0.34375\n",
      "epoch 79  -> loss: 0.43618696 | rewards: (train) 2.45562458 (test) 2.71749973 | success rate: 0.34375\n",
      "epoch 80  -> loss: 0.39348019 | rewards: (train) 2.31773448 (test) 2.71031213 | success rate: 0.375\n",
      "epoch 81  -> loss: 0.47674807 | rewards: (train) 2.22218728 (test) 2.1240623 | success rate: 0.21875\n",
      "epoch 82  -> loss: 0.40875047 | rewards: (train) 2.26648402 (test) 1.97593737 | success rate: 0.21875\n",
      "epoch 83  -> loss: 0.48744623 | rewards: (train) 2.13578105 (test) 1.85968745 | success rate: 0.1875\n",
      "epoch 84  -> loss: 0.53040442 | rewards: (train) 1.9592967 (test) 2.26281238 | success rate: 0.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 85  -> loss: 0.52485349 | rewards: (train) 1.86374986 (test) 2.25718713 | success rate: 0.28125\n",
      "epoch 86  -> loss: 0.54865066 | rewards: (train) 1.67164063 (test) 1.91374969 | success rate: 0.1875\n",
      "epoch 87  -> loss: 0.51204268 | rewards: (train) 1.95718741 (test) 2.10249996 | success rate: 0.25\n",
      "epoch 88  -> loss: 0.49698173 | rewards: (train) 1.98234367 (test) 1.13156259 | success rate: 0.03125\n",
      "epoch 89  -> loss: 0.45965649 | rewards: (train) 2.18765593 (test) 1.83124995 | success rate: 0.1875\n",
      "epoch 90  -> loss: 0.43871812 | rewards: (train) 1.94617176 (test) 2.19031215 | success rate: 0.28125\n",
      "epoch 91  -> loss: 0.4786575 | rewards: (train) 1.7335937 (test) 2.06843734 | success rate: 0.21875\n",
      "epoch 92  -> loss: 0.48932381 | rewards: (train) 1.97453105 (test) 2.18624997 | success rate: 0.25\n",
      "epoch 93  -> loss: 0.49444882 | rewards: (train) 1.64187503 (test) 1.67312491 | success rate: 0.125\n",
      "epoch 94  -> loss: 0.58732072 | rewards: (train) 1.81648421 (test) 2.49749947 | success rate: 0.3125\n",
      "epoch 95  -> loss: 0.61718006 | rewards: (train) 2.12726545 (test) 2.07749987 | success rate: 0.21875\n",
      "epoch 96  -> loss: 0.55432024 | rewards: (train) 2.06453109 (test) 2.10093713 | success rate: 0.25\n",
      "epoch 97  -> loss: 0.57149426 | rewards: (train) 2.18242168 (test) 2.16125011 | success rate: 0.25\n",
      "epoch 98  -> loss: 0.47210799 | rewards: (train) 2.01062465 (test) 1.79562497 | success rate: 0.09375\n",
      "epoch 99  -> loss: 0.43924137 | rewards: (train) 1.92874968 (test) 2.41874981 | success rate: 0.3125\n",
      "epoch 100  -> loss: 0.41581371 | rewards: (train) 1.80359364 (test) 2.18906236 | success rate: 0.28125\n",
      "epoch 101  -> loss: 0.45715937 | rewards: (train) 1.68773425 (test) 2.13062477 | success rate: 0.21875\n",
      "epoch 102  -> loss: 0.50697019 | rewards: (train) 2.01335907 (test) 1.94562483 | success rate: 0.25\n",
      "epoch 103  -> loss: 0.51336048 | rewards: (train) 1.81882799 (test) 1.67406237 | success rate: 0.21875\n",
      "epoch 104  -> loss: 0.51953326 | rewards: (train) 1.85054672 (test) 1.73562479 | success rate: 0.1875\n",
      "epoch 105  -> loss: 0.61052559 | rewards: (train) 1.63601542 (test) 1.92718732 | success rate: 0.25\n",
      "epoch 106  -> loss: 0.66556622 | rewards: (train) 1.76359355 (test) 1.34656239 | success rate: 0.125\n",
      "epoch 107  -> loss: 0.62517804 | rewards: (train) 2.44257784 (test) 2.25437498 | success rate: 0.25\n",
      "epoch 108  -> loss: 0.52775027 | rewards: (train) 2.15476561 (test) 3.04156208 | success rate: 0.5\n",
      "epoch 109  -> loss: 0.42990778 | rewards: (train) 2.32851529 (test) 2.95374966 | success rate: 0.46875\n",
      "epoch 110  -> loss: 0.38634179 | rewards: (train) 2.26007795 (test) 1.92156219 | success rate: 0.21875\n",
      "epoch 111  -> loss: 0.41529537 | rewards: (train) 2.25289035 (test) 2.16874957 | success rate: 0.3125\n",
      "epoch 112  -> loss: 0.48023507 | rewards: (train) 2.22312474 (test) 2.16906238 | success rate: 0.25\n",
      "epoch 113  -> loss: 0.55179688 | rewards: (train) 2.20898438 (test) 2.19718742 | success rate: 0.21875\n",
      "epoch 114  -> loss: 0.58831202 | rewards: (train) 2.47992158 (test) 2.81249976 | success rate: 0.40625\n",
      "epoch 115  -> loss: 0.53100573 | rewards: (train) 2.38523412 (test) 3.22031212 | success rate: 0.5\n",
      "epoch 116  -> loss: 0.51281425 | rewards: (train) 2.49945307 (test) 2.83093739 | success rate: 0.375\n",
      "epoch 117  -> loss: 0.50378174 | rewards: (train) 2.60671854 (test) 2.30968714 | success rate: 0.28125\n",
      "epoch 118  -> loss: 0.52925918 | rewards: (train) 2.37523413 (test) 2.97874999 | success rate: 0.40625\n",
      "epoch 119  -> loss: 0.42552256 | rewards: (train) 2.47703099 (test) 2.59593725 | success rate: 0.375\n",
      "epoch 120  -> loss: 0.45423277 | rewards: (train) 2.46609354 (test) 2.26062512 | success rate: 0.21875\n",
      "epoch 121  -> loss: 0.39558413 | rewards: (train) 2.49296856 (test) 1.95749986 | success rate: 0.21875\n",
      "epoch 122  -> loss: 0.42915255 | rewards: (train) 2.38046861 (test) 2.32375002 | success rate: 0.21875\n",
      "epoch 123  -> loss: 0.44717903 | rewards: (train) 2.11382794 (test) 3.13531208 | success rate: 0.46875\n",
      "epoch 124  -> loss: 0.45986955 | rewards: (train) 2.24867177 (test) 2.89437485 | success rate: 0.40625\n",
      "epoch 125  -> loss: 0.51966183 | rewards: (train) 2.28617191 (test) 2.76062465 | success rate: 0.3125\n",
      "epoch 126  -> loss: 0.50746641 | rewards: (train) 2.64124966 (test) 2.54156208 | success rate: 0.34375\n",
      "epoch 127  -> loss: 0.50986008 | rewards: (train) 2.74640608 (test) 2.80406213 | success rate: 0.34375\n",
      "epoch 128  -> loss: 0.4769959 | rewards: (train) 2.76085901 (test) 2.40093732 | success rate: 0.1875\n",
      "epoch 129  -> loss: 0.52342562 | rewards: (train) 2.55726552 (test) 2.74343729 | success rate: 0.375\n",
      "epoch 130  -> loss: 0.62100615 | rewards: (train) 2.67523408 (test) 2.74718714 | success rate: 0.3125\n",
      "epoch 131  -> loss: 0.56762522 | rewards: (train) 3.16796851 (test) 3.16718745 | success rate: 0.40625\n",
      "epoch 132  -> loss: 0.49957367 | rewards: (train) 3.68656206 (test) 3.47906208 | success rate: 0.46875\n",
      "epoch 133  -> loss: 0.3930884 | rewards: (train) 3.9589057 (test) 4.2249999 | success rate: 0.75\n",
      "epoch 134  -> loss: 0.33457563 | rewards: (train) 3.93242121 (test) 4.48218679 | success rate: 0.84375\n",
      "epoch 135  -> loss: 0.30716184 | rewards: (train) 4.17882776 (test) 4.60906124 | success rate: 0.90625\n",
      "epoch 136  -> loss: 0.26719647 | rewards: (train) 4.1603117 (test) 4.59999943 | success rate: 0.90625\n",
      "epoch 137  -> loss: 0.21561682 | rewards: (train) 4.25468683 (test) 4.72718668 | success rate: 0.96875\n",
      "epoch 138  -> loss: 0.2080732 | rewards: (train) 4.46460867 (test) 4.75687456 | success rate: 1.0\n",
      "epoch 139  -> loss: 0.20932033 | rewards: (train) 4.53249931 (test) 4.78249979 | success rate: 1.0\n",
      "epoch 140  -> loss: 0.17346779 | rewards: (train) 4.28038979 (test) 4.78124905 | success rate: 1.0\n",
      "epoch 141  -> loss: 0.16606662 | rewards: (train) 4.46109295 (test) 4.75843668 | success rate: 1.0\n",
      "epoch 142  -> loss: 0.18168685 | rewards: (train) 4.17820263 (test) 4.78937435 | success rate: 1.0\n",
      "epoch 143  -> loss: 0.19371256 | rewards: (train) 4.40820265 (test) 4.77218723 | success rate: 1.0\n",
      "epoch 144  -> loss: 0.15551276 | rewards: (train) 4.41093636 (test) 4.78031206 | success rate: 1.0\n",
      "epoch 145  -> loss: 0.15583438 | rewards: (train) 4.51265574 (test) 4.75937414 | success rate: 1.0\n",
      "epoch 146  -> loss: 0.19722876 | rewards: (train) 4.22601509 (test) 4.76249933 | success rate: 1.0\n",
      "epoch 147  -> loss: 0.14739051 | rewards: (train) 4.36148357 (test) 4.75749922 | success rate: 1.0\n",
      "epoch 148  -> loss: 0.20147382 | rewards: (train) 4.33515549 (test) 4.76062393 | success rate: 1.0\n",
      "epoch 149  -> loss: 0.17722209 | rewards: (train) 4.21140575 (test) 4.74906158 | success rate: 1.0\n",
      "epoch 150  -> loss: 0.20900263 | rewards: (train) 4.42640591 (test) 4.62156153 | success rate: 0.9375\n",
      "epoch 151  -> loss: 0.1997297 | rewards: (train) 4.36531162 (test) 4.74781179 | success rate: 1.0\n",
      "epoch 152  -> loss: 0.2316346 | rewards: (train) 4.43335867 (test) 4.74124908 | success rate: 1.0\n",
      "epoch 153  -> loss: 0.158318 | rewards: (train) 4.5335145 (test) 4.76156187 | success rate: 1.0\n",
      "epoch 154  -> loss: 0.14372875 | rewards: (train) 4.34132767 (test) 4.74406195 | success rate: 1.0\n",
      "epoch 155  -> loss: 0.18752274 | rewards: (train) 4.38414001 (test) 4.73437452 | success rate: 1.0\n",
      "epoch 156  -> loss: 0.16838578 | rewards: (train) 4.27062416 (test) 4.74562407 | success rate: 1.0\n",
      "epoch 157  -> loss: 0.1397978 | rewards: (train) 4.57117081 (test) 4.75374937 | success rate: 1.0\n",
      "epoch 158  -> loss: 0.12691982 | rewards: (train) 4.36960888 (test) 4.7524991 | success rate: 1.0\n",
      "epoch 159  -> loss: 0.1262542 | rewards: (train) 4.33624935 (test) 4.74093723 | success rate: 1.0\n",
      "epoch 160  -> loss: 0.12656451 | rewards: (train) 4.25468683 (test) 4.73656178 | success rate: 1.0\n",
      "epoch 161  -> loss: 0.17251445 | rewards: (train) 4.21968699 (test) 4.75531149 | success rate: 1.0\n",
      "epoch 162  -> loss: 0.15228728 | rewards: (train) 4.47562456 (test) 4.72718716 | success rate: 1.0\n",
      "epoch 163  -> loss: 0.14960843 | rewards: (train) 4.25062466 (test) 4.75249958 | success rate: 1.0\n",
      "epoch 164  -> loss: 0.17018623 | rewards: (train) 4.43914032 (test) 4.55593681 | success rate: 0.90625\n",
      "epoch 165  -> loss: 0.14906209 | rewards: (train) 4.35406208 (test) 4.72937441 | success rate: 1.0\n",
      "epoch 166  -> loss: 0.1507046 | rewards: (train) 4.25273371 (test) 4.74281168 | success rate: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 167  -> loss: 0.18450304 | rewards: (train) 4.26874924 (test) 4.55499935 | success rate: 0.90625\n",
      "epoch 168  -> loss: 0.17605906 | rewards: (train) 4.41789007 (test) 4.69656181 | success rate: 0.96875\n",
      "epoch 169  -> loss: 0.1869482 | rewards: (train) 4.38585854 (test) 4.69437456 | success rate: 0.96875\n",
      "epoch 170  -> loss: 0.15604835 | rewards: (train) 4.48945236 (test) 4.75906181 | success rate: 1.0\n",
      "epoch 171  -> loss: 0.16485157 | rewards: (train) 4.5838275 (test) 4.78093624 | success rate: 1.0\n",
      "epoch 172  -> loss: 0.14464759 | rewards: (train) 4.54703045 (test) 4.762187 | success rate: 1.0\n",
      "epoch 173  -> loss: 0.16335853 | rewards: (train) 4.58195257 (test) 4.746562 | success rate: 1.0\n",
      "epoch 174  -> loss: 0.15290919 | rewards: (train) 4.54913998 (test) 4.75062418 | success rate: 1.0\n",
      "epoch 175  -> loss: 0.15245875 | rewards: (train) 4.36851501 (test) 4.76343679 | success rate: 1.0\n",
      "epoch 176  -> loss: 0.15593816 | rewards: (train) 4.51757765 (test) 4.64531183 | success rate: 0.9375\n",
      "epoch 177  -> loss: 0.1638599 | rewards: (train) 4.42429638 (test) 4.76249933 | success rate: 1.0\n",
      "epoch 178  -> loss: 0.14494972 | rewards: (train) 4.60601521 (test) 4.63562441 | success rate: 0.9375\n",
      "epoch 179  -> loss: 0.17295987 | rewards: (train) 4.48687458 (test) 4.75031185 | success rate: 1.0\n",
      "epoch 180  -> loss: 0.13647714 | rewards: (train) 4.43781185 (test) 4.7543745 | success rate: 1.0\n",
      "epoch 181  -> loss: 0.17121849 | rewards: (train) 4.45796824 (test) 4.74718666 | success rate: 1.0\n",
      "epoch 182  -> loss: 0.17219325 | rewards: (train) 4.38929653 (test) 4.75718689 | success rate: 1.0\n",
      "epoch 183  -> loss: 0.20657646 | rewards: (train) 4.6057024 (test) 4.76499939 | success rate: 1.0\n",
      "epoch 184  -> loss: 0.19328501 | rewards: (train) 4.68953037 (test) 4.76468658 | success rate: 1.0\n",
      "epoch 185  -> loss: 0.16903559 | rewards: (train) 4.58546829 (test) 4.75187397 | success rate: 1.0\n",
      "epoch 186  -> loss: 0.17453921 | rewards: (train) 4.36179638 (test) 4.74999952 | success rate: 1.0\n",
      "epoch 187  -> loss: 0.17511396 | rewards: (train) 4.55757809 (test) 4.76406193 | success rate: 1.0\n",
      "epoch 188  -> loss: 0.17655999 | rewards: (train) 4.44531202 (test) 4.76562405 | success rate: 1.0\n",
      "epoch 189  -> loss: 0.15199619 | rewards: (train) 4.5041399 (test) 4.75999928 | success rate: 1.0\n",
      "epoch 190  -> loss: 0.16262298 | rewards: (train) 4.49984312 (test) 4.70343685 | success rate: 0.96875\n",
      "epoch 191  -> loss: 0.14254673 | rewards: (train) 4.48789024 (test) 4.62937403 | success rate: 0.9375\n",
      "epoch 192  -> loss: 0.16205488 | rewards: (train) 4.52476501 (test) 4.6606245 | success rate: 0.9375\n",
      "epoch 193  -> loss: 0.16090222 | rewards: (train) 4.52078056 (test) 4.76687431 | success rate: 1.0\n",
      "epoch 194  -> loss: 0.1530777 | rewards: (train) 4.61718655 (test) 4.71218681 | success rate: 0.96875\n",
      "epoch 195  -> loss: 0.12512707 | rewards: (train) 4.62609339 (test) 4.68093681 | success rate: 0.96875\n",
      "epoch 196  -> loss: 0.17690129 | rewards: (train) 4.3949213 (test) 4.762187 | success rate: 1.0\n",
      "epoch 197  -> loss: 0.13955817 | rewards: (train) 4.42624903 (test) 4.75312424 | success rate: 1.0\n",
      "epoch 198  -> loss: 0.14241866 | rewards: (train) 4.56468678 (test) 4.74624968 | success rate: 1.0\n",
      "epoch 199  -> loss: 0.11941535 | rewards: (train) 4.56953049 (test) 4.73781204 | success rate: 1.0\n",
      "epoch 200  -> loss: 0.16519966 | rewards: (train) 4.465312 (test) 4.69593668 | success rate: 0.96875\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_epoch):\n",
    "    qf.train(False)\n",
    "    paths = expl_path_collector.collect_new_paths(n_samples, max_len, False)\n",
    "    train_r = mean_reward(paths)\n",
    "    avg_r_train.append(train_r)\n",
    "    replay_buffer.add_paths(paths)\n",
    "    \n",
    "    paths = eval_path_collector.collect_new_paths(n_samples//4, max_len, False)\n",
    "    eval_r = mean_reward(paths)\n",
    "    avg_r_eval.append(eval_r)\n",
    "    \n",
    "    success_rate = np.mean([p['success'] for p in paths])\n",
    "    success_rates.append(success_rate)\n",
    "\n",
    "    qf.train(True)\n",
    "    for _ in range(n_iter):\n",
    "        batch = replay_buffer.random_batch(batch_size)\n",
    "\n",
    "        rewards = torch.tensor([b.r for b in batch])\n",
    "        terminals =torch.tensor([b.d for b in batch]).float()\n",
    "        actions = torch.tensor([b.a for b in batch])\n",
    "        \n",
    "        states = batch_graphs([b.s for b in batch])\n",
    "        next_s = batch_graphs([b.sp for b in batch])        \n",
    "\n",
    "        out = target_qf(next_s) # shape = (|G|, |J|, |W|)\n",
    "        target_q_values = get_scores(next_s, out)\n",
    "        y_target = rewards + (1. - terminals) * gamma * target_q_values \n",
    "        \n",
    "        out = qf(states)\n",
    "        y_pred = out[torch.arange(batch_size), actions.T[1], actions.T[0]]\n",
    "        qf_loss = qf_criterion(y_pred, y_target).to(torch.float)\n",
    "\n",
    "        loss.append(qf_loss.item())\n",
    "        avg_r_train.append(rewards.mean().item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    target_qf.load_state_dict(deepcopy(qf.state_dict()))\n",
    "    err = 8\n",
    "    print(\"epoch\", i+1, #\"| lr:\", scientific_notation(optimizer.param_groups[0][\"lr\"]) ,\n",
    "          \" -> loss:\", round(np.mean(loss[-n_iter:]), err),\n",
    "          \"| rewards: (train)\", round(train_r, err), \"(test)\", round(eval_r, err),\n",
    "          \"| success rate:\", round(success_rate, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a412f1c",
   "metadata": {},
   "source": [
    "```python\n",
    "g1 = env.reset()\n",
    "qf.eval()\n",
    "print(g1.edges(etype='precede'))\n",
    "print(qf(g1))\n",
    "\n",
    "g2, r, d, info = env2.step((0, 1))\n",
    "\n",
    "# paths = eval_path_collector.collect_new_paths(10, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785bb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlpyt] *",
   "language": "python",
   "name": "conda-env-rlpyt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
