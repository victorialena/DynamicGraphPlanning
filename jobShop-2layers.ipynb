{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6024c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4c0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/victorialena/rlkit')\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d59036ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from copy import copy, deepcopy\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cce6dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victorialena/anaconda3/envs/rlpyt/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/home/victorialena/anaconda3/envs/rlpyt/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Sized\n"
     ]
    }
   ],
   "source": [
    "from env.job_shop import jobShopScheduling\n",
    "from actor.job_shop import *\n",
    "from utils.job_shop import * # custom replay buffer and episode sampler\n",
    "from utils.jsp_makespan import *\n",
    "\n",
    "from path_collector import MdpPathCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1050e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "class hgnn(nn.Module):\n",
    "    def __init__(self, embedding_dim=16, k=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = dglnn.HeteroLinear({'job': 7, 'worker':3}, embedding_dim)\n",
    "        self.conv = dglnn.HeteroGraphConv({\n",
    "            'precede' : dglnn.GraphConv(embedding_dim, embedding_dim),\n",
    "            'next' : dglnn.GraphConv(embedding_dim, embedding_dim),\n",
    "            'processing' : dglnn.SAGEConv((embedding_dim, embedding_dim), embedding_dim, 'mean')},\n",
    "            aggregate='sum')\n",
    "               \n",
    "        self.pred = dotProductPredictor()\n",
    "        self.num_loops = k\n",
    "        \n",
    "    def forward(self, g):\n",
    "        h0 = {**g.ndata['hv'], **g.ndata['he']}\n",
    "        hv = self.embedding(h0)\n",
    "        hw = hv['worker']\n",
    "        for _ in range(self.num_loops):\n",
    "            hv = {'job': self.conv(g, hv)['job'],\n",
    "                  'worker': hw}\n",
    "            \n",
    "        rg = construct_readout_graph(g, ('worker', 'processing', 'job'))\n",
    "        return self.pred(rg, hv['job'], hw, ('worker', 'processing', 'job'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f4cb18",
   "metadata": {},
   "source": [
    "#### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7516976",
   "metadata": {},
   "outputs": [],
   "source": [
    "scientific_notation =  lambda x:\"{:.2e}\".format(x)\n",
    "\n",
    "def get_scores(g, scores):\n",
    "    n = scores.shape[0]\n",
    "    idx = (g.ndata['hv']['job'][:, 3] == 0).view(n, -1)\n",
    "    \n",
    "    values, workers = scores.max(-1, keepdims=False)\n",
    "    return torch.stack([values[i][idx[i]].max() if sum(idx[i]).item()>0 else torch.tensor(0.) for i in range(n)])\n",
    "\n",
    "def mean_reward(paths):\n",
    "    return torch.tensor([p['rewards'] for p in paths]).sum(1).mean().item()\n",
    "\n",
    "def mean_makespan(paths):\n",
    "    \"Returns the average makespan successful paths from given list. Returns *nan* if no path was successful.\"\n",
    "    return torch.tensor([p['makespan'] for p in paths if p['success']]).mean().item()\n",
    "\n",
    "def relative_makespan_error(paths):\n",
    "    \"\"\" From initial conditions of each path, evaluate optimal makespan for each path and compare against\n",
    "    sampled trajectory. \"\"\"\n",
    "    err = []\n",
    "    for p in paths:\n",
    "        if not p['success']:\n",
    "            continue\n",
    "        jdata = g2jobdata(p['observations'][0], p['actions'])\n",
    "        makespan, status = get_makespan(jdata)\n",
    "        if makespan > -1: # feasible\n",
    "            relative_error = p['makespan']/makespan - 1\n",
    "            err.append(relative_error)\n",
    "        else: \n",
    "            print(\"This should not be possible, check for bugs!!\")\n",
    "            \n",
    "    return torch.tensor(err).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcf7123",
   "metadata": {},
   "source": [
    "#### Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5cd507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cd90418",
   "metadata": {},
   "outputs": [],
   "source": [
    "njobs, nworkers = 25, 10 # 10, 2 #5, 2\n",
    "env = jobShopScheduling(njobs, nworkers)\n",
    "g0 = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d3e2f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "layersz = 16\n",
    "\n",
    "qf = hgnn(layersz)\n",
    "expl_policy = epsilonGreedyPolicy(qf, .1)\n",
    "\n",
    "target_qf = hgnn(layersz)\n",
    "eval_policy = epsilonGreedyPolicy(target_qf, 0.)\n",
    "\n",
    "expl_path_collector = MdpPathCollector(env, expl_policy, rollout_fn=sample_episode, parallelize=False)\n",
    "eval_path_collector = MdpPathCollector(env, eval_policy, rollout_fn=sample_episode, parallelize=False)\n",
    "\n",
    "replay_buffer_cap = 5000 #10000\n",
    "replay_buffer = replayBuffer(replay_buffer_cap, prioritized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbace81e",
   "metadata": {},
   "source": [
    "```python\n",
    "path = rollout(env, expl_policy, 2500)\n",
    "path['terminals'][-1]\n",
    "env.render()\n",
    "\n",
    "replay_buffer.add_path(path, env.g)\n",
    "\n",
    "replay_buffer.random_batch(50)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b02caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 8e-5 #1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a116e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(qf.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "qf_criterion = nn.MSELoss()\n",
    "\n",
    "max_len = njobs+1\n",
    "n_samples = 128 \n",
    "n_epoch = 400 #300 #425 #300 #200\n",
    "n_iter = 64\n",
    "batch_size = 64 #32\n",
    "gamma = 1.0\n",
    "\n",
    "loss = []\n",
    "avg_r_train = []\n",
    "avg_r_eval = []\n",
    "success_rates = []\n",
    "relative_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00003bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victorialena/anaconda3/envs/rlpyt/lib/python3.7/site-packages/dgl/heterograph.py:354: DGLWarning: DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\n",
      "  dgl_warning(\"DGLGraph.add_edge is deprecated. Please use DGLGraph.add_edges\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  -> Loss: 719.501 | Rewards: (train) 11.893 (test) -0.44 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 2  -> Loss: 186.599 | Rewards: (train) 3.998 (test) 4.06 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 3  -> Loss: 212.482 | Rewards: (train) 4.606 (test) 4.101 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 4  -> Loss: 229.917 | Rewards: (train) 4.692 (test) 3.848 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 5  -> Loss: 108.61 | Rewards: (train) 4.518 (test) 3.76 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 6  -> Loss: 98.464 | Rewards: (train) 4.08 (test) 4.561 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 7  -> Loss: 128.805 | Rewards: (train) 3.74 (test) 2.693 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 8  -> Loss: 87.335 | Rewards: (train) 3.84 (test) 3.118 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 9  -> Loss: 80.239 | Rewards: (train) 3.634 (test) 3.483 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 10  -> Loss: 53.574 | Rewards: (train) 2.778 (test) 1.563 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 11  -> Loss: 70.885 | Rewards: (train) 3.015 (test) 2.61 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 12  -> Loss: 69.283 | Rewards: (train) 3.359 (test) 2.277 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 13  -> Loss: 53.89 | Rewards: (train) 2.944 (test) 1.598 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 14  -> Loss: 61.535 | Rewards: (train) 2.536 (test) 0.847 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 15  -> Loss: 54.104 | Rewards: (train) 2.168 (test) 1.182 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 16  -> Loss: 71.346 | Rewards: (train) 2.003 (test) 0.857 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 17  -> Loss: 55.708 | Rewards: (train) 1.99 (test) 0.363 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 18  -> Loss: 48.727 | Rewards: (train) 1.705 (test) 0.853 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 19  -> Loss: 46.481 | Rewards: (train) 1.71 (test) 0.478 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 20  -> Loss: 50.054 | Rewards: (train) 1.62 (test) 0.113 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 21  -> Loss: 45.407 | Rewards: (train) 1.477 (test) 0.082 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 22  -> Loss: 60.642 | Rewards: (train) 1.65 (test) 0.113 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 23  -> Loss: 29.44 | Rewards: (train) 2.165 (test) 0.083 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 24  -> Loss: 38.291 | Rewards: (train) 1.537 (test) 0.265 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 25  -> Loss: 51.614 | Rewards: (train) 1.714 (test) -0.132 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 26  -> Loss: 40.219 | Rewards: (train) 0.964 (test) -0.163 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 27  -> Loss: 33.912 | Rewards: (train) 0.956 (test) -0.286 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 28  -> Loss: 34.497 | Rewards: (train) 0.946 (test) -0.133 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 29  -> Loss: 33.2 | Rewards: (train) 1.406 (test) 0.145 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 30  -> Loss: 29.18 | Rewards: (train) 1.317 (test) 0.109 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 31  -> Loss: 27.33 | Rewards: (train) 0.883 (test) -0.193 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 32  -> Loss: 32.973 | Rewards: (train) 0.986 (test) 0.113 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 33  -> Loss: 30.684 | Rewards: (train) 1.262 (test) 0.298 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 34  -> Loss: 23.377 | Rewards: (train) 0.824 (test) 0.138 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 35  -> Loss: 21.976 | Rewards: (train) 1.08 (test) -0.166 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 36  -> Loss: 20.017 | Rewards: (train) 1.158 (test) 0.198 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 37  -> Loss: 11.464 | Rewards: (train) 1.474 (test) 0.446 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 38  -> Loss: 20.995 | Rewards: (train) 0.638 (test) -0.232 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 39  -> Loss: 17.923 | Rewards: (train) 0.532 (test) -0.256 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 40  -> Loss: 27.144 | Rewards: (train) 0.819 (test) 0.406 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 41  -> Loss: 16.572 | Rewards: (train) 1.168 (test) -0.138 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 42  -> Loss: 15.37 | Rewards: (train) 0.742 (test) -0.167 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 43  -> Loss: 22.078 | Rewards: (train) 1.024 (test) -0.288 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 44  -> Loss: 15.937 | Rewards: (train) 0.641 (test) -0.104 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 45  -> Loss: 10.737 | Rewards: (train) 0.99 (test) -0.167 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 46  -> Loss: 7.633 | Rewards: (train) 1.207 (test) -0.048 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 47  -> Loss: 4.836 | Rewards: (train) 1.224 (test) 0.322 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 48  -> Loss: 40.964 | Rewards: (train) 0.14 (test) -0.5 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 49  -> Loss: 44.145 | Rewards: (train) 0.023 (test) -0.5 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 50  -> Loss: 46.991 | Rewards: (train) 0.218 (test) -0.44 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 51  -> Loss: 38.637 | Rewards: (train) 0.177 (test) -0.439 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 52  -> Loss: 30.681 | Rewards: (train) 0.233 (test) -0.471 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 53  -> Loss: 42.554 | Rewards: (train) 0.206 (test) -0.412 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 54  -> Loss: 42.971 | Rewards: (train) 0.629 (test) -0.177 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 55  -> Loss: 57.079 | Rewards: (train) 0.665 (test) 0.003 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 56  -> Loss: 55.283 | Rewards: (train) 0.913 (test) 0.034 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 57  -> Loss: 67.991 | Rewards: (train) 1.011 (test) 0.235 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 58  -> Loss: 92.3 | Rewards: (train) 1.265 (test) 0.368 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 59  -> Loss: 67.438 | Rewards: (train) 1.254 (test) 0.636 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 60  -> Loss: 64.943 | Rewards: (train) 1.255 (test) 0.508 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 61  -> Loss: 44.776 | Rewards: (train) 1.402 (test) 0.816 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 62  -> Loss: 45.749 | Rewards: (train) 1.766 (test) 1.265 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 63  -> Loss: 9.067 | Rewards: (train) 1.791 (test) 1.36 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 64  -> Loss: 41.951 | Rewards: (train) 1.869 (test) 2.021 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 65  -> Loss: 24.833 | Rewards: (train) 2.152 (test) 2.081 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 66  -> Loss: 49.777 | Rewards: (train) 2.586 (test) 1.133 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 67  -> Loss: 33.211 | Rewards: (train) 1.662 (test) 1.762 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 68  -> Loss: 1.821 | Rewards: (train) 1.743 (test) 1.334 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 69  -> Loss: 39.909 | Rewards: (train) 1.599 (test) 1.246 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 70  -> Loss: 45.938 | Rewards: (train) 1.361 (test) 0.27 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71  -> Loss: 7.238 | Rewards: (train) 2.088 (test) 1.58 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 72  -> Loss: 4.598 | Rewards: (train) 2.568 (test) 2.309 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 73  -> Loss: 4.137 | Rewards: (train) 3.218 (test) 3.231 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 74  -> Loss: 5.183 | Rewards: (train) 2.274 (test) 2.425 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 75  -> Loss: 5.511 | Rewards: (train) 3.096 (test) 2.523 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 76  -> Loss: 3.054 | Rewards: (train) 2.978 (test) 3.316 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 77  -> Loss: 14.732 | Rewards: (train) 2.768 (test) 2.301 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 78  -> Loss: 3.952 | Rewards: (train) 2.185 (test) 1.198 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 79  -> Loss: 6.352 | Rewards: (train) 2.633 (test) 2.145 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 80  -> Loss: 74.592 | Rewards: (train) 1.857 (test) 1.404 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 81  -> Loss: 73.524 | Rewards: (train) 2.16 (test) 0.847 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 82  -> Loss: 33.461 | Rewards: (train) 2.079 (test) 1.585 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 83  -> Loss: 47.382 | Rewards: (train) 2.361 (test) 2.341 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 84  -> Loss: 18.204 | Rewards: (train) 2.543 (test) 3.139 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 85  -> Loss: 3.986 | Rewards: (train) 2.858 (test) 2.524 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 86  -> Loss: 4.307 | Rewards: (train) 3.598 (test) 2.801 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 87  -> Loss: 12.383 | Rewards: (train) 3.382 (test) 3.102 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 88  -> Loss: 4.119 | Rewards: (train) 4.006 (test) 3.904 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 89  -> Loss: 16.074 | Rewards: (train) 3.552 (test) 5.107 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 90  -> Loss: 2.676 | Rewards: (train) 3.679 (test) 4.337 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 91  -> Loss: 11.564 | Rewards: (train) 3.448 (test) 4.477 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 92  -> Loss: 8.065 | Rewards: (train) 4.372 (test) 4.353 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 93  -> Loss: 2.302 | Rewards: (train) 4.155 (test) 4.658 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 94  -> Loss: 2.172 | Rewards: (train) 2.943 (test) 3.005 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 95  -> Loss: 4.664 | Rewards: (train) 3.391 (test) 2.845 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 96  -> Loss: 1.461 | Rewards: (train) 3.679 (test) 3.228 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 97  -> Loss: 10.954 | Rewards: (train) 3.044 (test) 3.654 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 98  -> Loss: 2.335 | Rewards: (train) 4.086 (test) 3.902 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 99  -> Loss: 1.233 | Rewards: (train) 3.429 (test) 2.19 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 100  -> Loss: 5.018 | Rewards: (train) 3.867 (test) 4.126 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 101  -> Loss: 1.806 | Rewards: (train) 2.393 (test) 2.392 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 102  -> Loss: 105.216 | Rewards: (train) 1.454 (test) 1.452 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 103  -> Loss: 63.961 | Rewards: (train) 1.412 (test) 1.36 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 104  -> Loss: 29.157 | Rewards: (train) 2.318 (test) 2.4 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 105  -> Loss: 9.208 | Rewards: (train) 3.52 (test) 3.022 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 106  -> Loss: 1.87 | Rewards: (train) 3.184 (test) 3.086 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 107  -> Loss: 6.818 | Rewards: (train) 3.954 (test) 4.538 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 108  -> Loss: 0.98 | Rewards: (train) 4.495 (test) 3.705 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 109  -> Loss: 3.406 | Rewards: (train) 4.158 (test) 5.403 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 110  -> Loss: 10.179 | Rewards: (train) 6.118 (test) 6.143 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 111  -> Loss: 1.173 | Rewards: (train) 4.613 (test) 4.76 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 112  -> Loss: 21.972 | Rewards: (train) 4.53 (test) 4.548 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 113  -> Loss: 272.808 | Rewards: (train) 0.546 (test) -0.5 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 114  -> Loss: 274.288 | Rewards: (train) 0.912 (test) -0.5 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 115  -> Loss: 301.393 | Rewards: (train) 1.019 (test) -0.5 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 116  -> Loss: 257.237 | Rewards: (train) 0.9 (test) -0.178 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 117  -> Loss: 277.471 | Rewards: (train) 1.049 (test) 0.117 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 118  -> Loss: 235.815 | Rewards: (train) 1.197 (test) 0.477 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 119  -> Loss: 205.779 | Rewards: (train) 1.629 (test) 0.913 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 120  -> Loss: 209.713 | Rewards: (train) 1.697 (test) 0.676 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 121  -> Loss: 206.1 | Rewards: (train) 1.541 (test) 1.376 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 122  -> Loss: 129.017 | Rewards: (train) 2.086 (test) 2.054 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 123  -> Loss: 185.822 | Rewards: (train) 2.039 (test) 1.789 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 124  -> Loss: 207.796 | Rewards: (train) 2.547 (test) 2.192 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 125  -> Loss: 70.151 | Rewards: (train) 2.427 (test) 2.61 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 126  -> Loss: 145.467 | Rewards: (train) 2.704 (test) 3.139 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 127  -> Loss: 17.965 | Rewards: (train) 2.961 (test) 2.647 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 128  -> Loss: 7.141 | Rewards: (train) 2.461 (test) 2.588 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 129  -> Loss: 18.863 | Rewards: (train) 2.744 (test) 3.52 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 130  -> Loss: 30.003 | Rewards: (train) 3.102 (test) 3.18 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 131  -> Loss: 0.979 | Rewards: (train) 3.219 (test) 4.098 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 132  -> Loss: 0.605 | Rewards: (train) 3.557 (test) 3.007 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 133  -> Loss: 9.522 | Rewards: (train) 2.694 (test) 2.91 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 134  -> Loss: 6.846 | Rewards: (train) 3.271 (test) 4.837 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 135  -> Loss: 6.002 | Rewards: (train) 4.345 (test) 3.191 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 136  -> Loss: 67.009 | Rewards: (train) 2.918 (test) 5.014 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 137  -> Loss: 3.954 | Rewards: (train) 4.404 (test) 4.957 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 138  -> Loss: 12.516 | Rewards: (train) 4.123 (test) 4.597 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 139  -> Loss: 2.249 | Rewards: (train) 4.391 (test) 5.569 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 140  -> Loss: 45.695 | Rewards: (train) 4.505 (test) 4.531 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141  -> Loss: 27.328 | Rewards: (train) 5.111 (test) 6.23 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 142  -> Loss: 0.633 | Rewards: (train) 6.3 (test) 7.172 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 143  -> Loss: 20.609 | Rewards: (train) 6.228 (test) 6.777 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 144  -> Loss: 2.054 | Rewards: (train) 6.506 (test) 7.835 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 145  -> Loss: 31.864 | Rewards: (train) 6.632 (test) 9.119 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 146  -> Loss: 0.528 | Rewards: (train) 5.177 (test) 7.627 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 147  -> Loss: 59.439 | Rewards: (train) 4.634 (test) 6.657 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 148  -> Loss: 41.436 | Rewards: (train) 11.634 (test) 14.349 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 149  -> Loss: 119.937 | Rewards: (train) 11.433 (test) 15.622 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 150  -> Loss: 7.087 | Rewards: (train) 4.824 (test) 7.654 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 151  -> Loss: 18.639 | Rewards: (train) 6.992 (test) 10.224 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 152  -> Loss: 9.407 | Rewards: (train) 11.948 (test) 14.301 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 153  -> Loss: 16.464 | Rewards: (train) 6.283 (test) 10.439 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 154  -> Loss: 15.517 | Rewards: (train) 13.179 (test) 15.815 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 155  -> Loss: 26.521 | Rewards: (train) 2.851 (test) 3.652 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 156  -> Loss: 2.528 | Rewards: (train) 6.472 (test) 8.821 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 157  -> Loss: 9.963 | Rewards: (train) 7.709 (test) 10.116 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 158  -> Loss: 30.945 | Rewards: (train) 6.924 (test) 10.352 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 159  -> Loss: 14.499 | Rewards: (train) 11.181 (test) 13.457 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 160  -> Loss: 19.905 | Rewards: (train) 11.494 (test) 13.946 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 161  -> Loss: 13.546 | Rewards: (train) 7.556 (test) 8.291 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 162  -> Loss: 12.1 | Rewards: (train) 12.577 (test) 14.124 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 163  -> Loss: 5.931 | Rewards: (train) 9.402 (test) 10.885 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 164  -> Loss: 14.635 | Rewards: (train) 9.417 (test) 12.445 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 165  -> Loss: 8.292 | Rewards: (train) 8.162 (test) 10.775 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 166  -> Loss: 1.256 | Rewards: (train) 13.378 (test) 16.567 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 167  -> Loss: 6.449 | Rewards: (train) 13.436 (test) 12.689 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 168  -> Loss: 0.508 | Rewards: (train) 9.797 (test) 10.166 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 169  -> Loss: 0.913 | Rewards: (train) 10.175 (test) 11.858 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 170  -> Loss: 6.949 | Rewards: (train) 11.979 (test) 13.235 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 171  -> Loss: 51.762 | Rewards: (train) 4.016 (test) 5.36 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 172  -> Loss: 119.479 | Rewards: (train) 4.511 (test) 4.721 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 173  -> Loss: 126.961 | Rewards: (train) 13.211 (test) 16.84 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 174  -> Loss: 316.615 | Rewards: (train) 2.548 (test) 2.273 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 175  -> Loss: 892.206 | Rewards: (train) 2.548 (test) 2.719 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 176  -> Loss: 489.165 | Rewards: (train) 2.203 (test) 1.521 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 177  -> Loss: 401.452 | Rewards: (train) 1.876 (test) 1.853 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 178  -> Loss: 276.054 | Rewards: (train) 2.496 (test) 1.921 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 179  -> Loss: 304.681 | Rewards: (train) 2.649 (test) 2.293 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 180  -> Loss: 152.349 | Rewards: (train) 2.576 (test) 2.537 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 181  -> Loss: 294.479 | Rewards: (train) 3.109 (test) 3.037 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 182  -> Loss: 221.008 | Rewards: (train) 3.271 (test) 2.941 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 183  -> Loss: 236.286 | Rewards: (train) 3.019 (test) 3.608 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 184  -> Loss: 119.755 | Rewards: (train) 3.188 (test) 3.714 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 185  -> Loss: 64.372 | Rewards: (train) 3.594 (test) 3.471 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 186  -> Loss: 98.179 | Rewards: (train) 3.634 (test) 3.753 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 187  -> Loss: 154.768 | Rewards: (train) 3.757 (test) 3.295 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 188  -> Loss: 15.607 | Rewards: (train) 4.006 (test) 3.913 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 189  -> Loss: 15.83 | Rewards: (train) 4.186 (test) 4.392 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 190  -> Loss: 33.275 | Rewards: (train) 4.264 (test) 4.804 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 191  -> Loss: 78.053 | Rewards: (train) 4.282 (test) 3.453 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 192  -> Loss: 47.785 | Rewards: (train) 4.149 (test) 4.547 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 193  -> Loss: 6.811 | Rewards: (train) 4.534 (test) 4.714 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 194  -> Loss: 15.748 | Rewards: (train) 4.657 (test) 4.694 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 195  -> Loss: 10.001 | Rewards: (train) 4.332 (test) 3.966 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 196  -> Loss: 6.997 | Rewards: (train) 5.055 (test) 4.931 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 197  -> Loss: 18.543 | Rewards: (train) 4.962 (test) 5.274 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 198  -> Loss: 22.003 | Rewards: (train) 4.568 (test) 4.368 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 199  -> Loss: 30.396 | Rewards: (train) 4.156 (test) 4.216 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 200  -> Loss: 24.257 | Rewards: (train) 4.33 (test) 6.019 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 201  -> Loss: 47.264 | Rewards: (train) 4.299 (test) 4.601 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 202  -> Loss: 14.943 | Rewards: (train) 4.062 (test) 3.936 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 203  -> Loss: 9.915 | Rewards: (train) 4.096 (test) 4.232 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 204  -> Loss: 11.617 | Rewards: (train) 4.439 (test) 4.471 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 205  -> Loss: 3.53 | Rewards: (train) 4.203 (test) 5.091 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 206  -> Loss: 13.046 | Rewards: (train) 4.01 (test) 5.435 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 207  -> Loss: 32.569 | Rewards: (train) 4.624 (test) 4.59 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 208  -> Loss: 11.228 | Rewards: (train) 4.666 (test) 4.331 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 209  -> Loss: 12.362 | Rewards: (train) 5.155 (test) 5.047 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210  -> Loss: 4.293 | Rewards: (train) 4.063 (test) 4.368 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 211  -> Loss: 2.091 | Rewards: (train) 4.785 (test) 3.998 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 212  -> Loss: 1.219 | Rewards: (train) 4.406 (test) 4.202 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 213  -> Loss: 25.549 | Rewards: (train) 4.093 (test) 4.552 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 214  -> Loss: 18.006 | Rewards: (train) 4.536 (test) 4.571 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 215  -> Loss: 54.431 | Rewards: (train) 4.771 (test) 5.013 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 216  -> Loss: 3.433 | Rewards: (train) 4.52 (test) 4.683 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n",
      "Epoch 217  -> Loss: 6.269 | Rewards: (train) 3.961 (test) 4.487 | Success rate: 0.0 | Makespan: nan | Rel. error: nan\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_epoch):\n",
    "    qf.train(False)\n",
    "    paths = expl_path_collector.collect_new_paths(n_samples, max_len, False)\n",
    "    train_r = mean_reward(paths)\n",
    "    avg_r_train.append(train_r)\n",
    "    replay_buffer.add_paths(paths)\n",
    "    \n",
    "    paths = eval_path_collector.collect_new_paths(n_samples//4, max_len, False)\n",
    "    eval_r = mean_reward(paths)\n",
    "    avg_r_eval.append(eval_r)\n",
    "    \n",
    "    success_rate = np.mean([p['success'] for p in paths])\n",
    "    success_rates.append(success_rate)\n",
    "    \n",
    "    avg_makespan = mean_makespan(paths)\n",
    "    relative_err = relative_makespan_error(paths)\n",
    "    relative_errors.append(relative_err)\n",
    "\n",
    "    qf.train(True)\n",
    "    if i==200:\n",
    "        expl_policy.eps = 0.05\n",
    "        \n",
    "    for _ in range(n_iter):\n",
    "        batch = replay_buffer.random_batch(batch_size)\n",
    "\n",
    "        rewards = torch.tensor([b.r for b in batch])\n",
    "        terminals = torch.tensor([b.d for b in batch]).float()\n",
    "        actions = torch.tensor([b.a for b in batch])\n",
    "        \n",
    "        states = batch_graphs([b.s for b in batch])\n",
    "        next_s = batch_graphs([b.sp for b in batch])        \n",
    "\n",
    "        out = target_qf(next_s) # shape = (|G|, |J|, |W|)\n",
    "        target_q_values = get_scores(next_s, out)\n",
    "        y_target = rewards + (1. - terminals) * gamma * target_q_values \n",
    "        \n",
    "        out = qf(states)\n",
    "        y_pred = out[torch.arange(batch_size), actions.T[1], actions.T[0]]\n",
    "        qf_loss = qf_criterion(y_pred, y_target).to(torch.float)\n",
    "\n",
    "        loss.append(qf_loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    target_qf.load_state_dict(deepcopy(qf.state_dict()))\n",
    "    err = 3\n",
    "    print(\"Epoch\", i+1, #\"| lr:\", scientific_notation(optimizer.param_groups[0][\"lr\"]) ,\n",
    "          \" -> Loss:\", round(np.mean(loss[-n_iter:]), err),\n",
    "          \"| Rewards: (train)\", round(train_r, err), \"(test)\", round(eval_r, err),\n",
    "          \"| Success rate:\", round(success_rate, err), \n",
    "          \"| Makespan:\", round(avg_makespan, err), \n",
    "          \"| Rel. error:\", round(relative_err, err), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de860c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_qf.eval()\n",
    "torch.save(target_qf.state_dict(), \"jobshop_qf_j%d-w%d_4x4_multilayer\" % (njobs, nworkers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bae796",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde34be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cc1c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [np.mean(loss[i*n_iter:(i+1)*n_iter]) for i in range(n_epoch)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d4bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(n_epoch)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot(x, losses)QQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQQ\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.subplot(222)\n",
    "plt.plot(x, avg_r_train, label=\"test\")\n",
    "plt.plot(x, avg_r_eval, 'r--', label=\"eval\")\n",
    "plt.legend()\n",
    "plt.ylabel('Train/Test Rewards [path, avg]')\n",
    "plt.xlabel('Epoch')\n",
    "plt.subplot(223)\n",
    "plt.plot(x, success_rates)\n",
    "plt.ylabel('Success Rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.subplot(224)\n",
    "plt.plot(x, [0.0]*len(x), 'lightgray', linestyle='--')\n",
    "plt.plot(x, relative_errors)\n",
    "plt.ylabel('Relative Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.suptitle('Training Performance Summary', y=.95)\n",
    "plt.savefig('figs/job_shop/j%d-w%d_4x4_multilayer' % (njobs, nworkers), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c4fd95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlpyt] *",
   "language": "python",
   "name": "conda-env-rlpyt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
