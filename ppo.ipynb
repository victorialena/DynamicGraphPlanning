{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c796605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/victorialena/rlkit')\n",
    "\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt\n",
    "import rlkit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from actor.collision_avoidance import *\n",
    "from actor.actor_critic import *\n",
    "from env.collision_avoidance import *\n",
    "from replay_buffer import PPOBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c13f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.seed = 0\n",
    "        self.c_hidden = [64, 8]\n",
    "        self.n_linear = 1\n",
    "        self.eps = 0.1 \n",
    "        self.sim_annealing_fac = 1.0 \n",
    "        self.replay_buffer_cap = 10000\n",
    "        self.n_samples = 10\n",
    "        self.prioritized_replay = True\n",
    "        self.learning_rate = 1E-4 #1E-3\n",
    "        self.n_epoch = 100 \n",
    "        self.n_iter = 128 \n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99\n",
    "        self.dropout = 0.05\n",
    "        self.max_ep_len = 1000\n",
    "\n",
    "        self.graph_type = \"full\" \n",
    "        self.load_from = \"\"\n",
    "        self.save_to = \"\"\n",
    "        self.plot = True\n",
    "        self.plot_name = \"\"\n",
    "        self.max_sample_distance = -1\n",
    "\n",
    "        self.maze_size = 5 \n",
    "        self.ndrones = 4\n",
    "        self.ngoals = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c0dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "device = 'cpu' #torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "env = collisionAvoidance(args, device=device)\n",
    "x = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c436db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "env.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f684157",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels, out_channels = env.get_channels()\n",
    "\n",
    "qf_v = collisionAvoidanceModel(in_channels, 1, args.c_hidden, n_linear=args.n_linear, \n",
    "                               bounds=env.get_size(), dropout=args.dropout)\n",
    "\n",
    "qf_pi = collisionAvoidanceModel(in_channels, out_channels, args.c_hidden, n_linear=args.n_linear, \n",
    "                                bounds=env.get_size(), dropout=args.dropout)\n",
    "\n",
    "ac = MLPActorCritic(qf_pi, qf_v);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca2e62b",
   "metadata": {},
   "source": [
    "```python\n",
    "lam=0.97\n",
    "target_kl=0.01\n",
    "clip_ratio=0.2\n",
    "pi_lr=3e-4\n",
    "vf_lr=1e-3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08d8b6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo(env, actor_critic, args, \n",
    "        lam=0.97, target_kl=0.01, clip_ratio=0.2, pi_lr=3e-4, vf_lr=1e-3):    \n",
    "    \"\"\" Proximal Policy Optimization (by clipping), with early stopping based on approximate KL \"\"\"\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    env.seed(args.seed)\n",
    "\n",
    "    def compute_loss_pi(data):\n",
    "        obs, act, adv, logp_old = data['obs'], data['act'], data['adv'].unsqueeze(1), data['logp']\n",
    "        act = act.reshape(-1, act.shape[-1])\n",
    "        \n",
    "        # Policy loss\n",
    "        pi, logp = actor_critic.pi(obs, act)\n",
    "        logp = logp.reshape(logp_old.shape)\n",
    "        \n",
    "        ratio = torch.exp(logp - logp_old)\n",
    "        clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv\n",
    "        loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "\n",
    "        # Useful extra info\n",
    "        approx_kl = (logp_old - logp).mean().item()\n",
    "        ent = pi.entropy().mean().item()\n",
    "        clipped = ratio.gt(1+clip_ratio) | ratio.lt(1-clip_ratio)\n",
    "        clipfrac = torch.as_tensor(clipped, dtype=torch.float32).mean().item()\n",
    "        pi_info = dict(kl=approx_kl, ent=ent, cf=clipfrac)\n",
    "\n",
    "        return loss_pi, pi_info\n",
    "\n",
    "    def compute_loss_v(data):\n",
    "        obs, ret = data['obs'], data['ret']\n",
    "        return ((actor_critic.v(obs).sum(1) - ret)**2).mean()\n",
    "\n",
    "    pi_optimizer = Adam(actor_critic.pi.parameters(), lr=pi_lr)\n",
    "    vf_optimizer = Adam(actor_critic.v.parameters(), lr=vf_lr)\n",
    "\n",
    "    def update():\n",
    "        # TODO: should this be in the the training loop?\n",
    "        data = buf.get()\n",
    "\n",
    "        pi_l_old, pi_info_old = compute_loss_pi(data)\n",
    "        pi_l_old = pi_l_old.item()\n",
    "        v_l_old = compute_loss_v(data).item()\n",
    "\n",
    "        # Train policy with multiple steps of gradient descent\n",
    "        for i in range(args.n_iter):\n",
    "            pi_optimizer.zero_grad()\n",
    "            loss_pi, pi_info = compute_loss_pi(data)\n",
    "            kl = pi_info['kl'] # mpi_avg(pi_info['kl'])\n",
    "            if kl > 1.5 * target_kl:\n",
    "                break # Early stopping at step %d due to reaching max kl\n",
    "            loss_pi.backward()\n",
    "            # mpi_avg_grads(actor_critic.pi)    # average grads across MPI processes\n",
    "            pi_optimizer.step()\n",
    "\n",
    "        # Value function learning\n",
    "        for i in range(args.n_iter):\n",
    "            vf_optimizer.zero_grad()\n",
    "            loss_v = compute_loss_v(data)\n",
    "            loss_v.backward()\n",
    "            # mpi_avg_grads(actor_critic.v)    # average grads across MPI processes\n",
    "            vf_optimizer.step()\n",
    "\n",
    "        # Log changes from update\n",
    "        # kl, ent, cf = pi_info['kl'], pi_info_old['ent'], pi_info['cf']\n",
    "        return loss_pi.item(), loss_v.item()\n",
    "\n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "    max_ep_len = args.max_ep_len\n",
    "    buf = PPOBuffer(args.n_samples*max_ep_len, # args.replay_buffer_cap, \n",
    "                    args.gamma, lam)\n",
    "\n",
    "    for epoch in range(args.n_epoch):        \n",
    "        actor_critic.train(True)\n",
    "        ret_train = []\n",
    "        for t in range(args.n_samples*max_ep_len):\n",
    "            if ep_len == 0:\n",
    "                buf.start_path()\n",
    "            a, v, logp = actor_critic.step(o)\n",
    "\n",
    "            next_o, r, d, _ = env.step(a)\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            # save and log\n",
    "            buf.add_sample(o, a, r, v, logp)\n",
    "\n",
    "            # Update obs (critical!)\n",
    "            o = next_o\n",
    "\n",
    "            timeout = ep_len == max_ep_len\n",
    "            terminal = d or timeout\n",
    "            epoch_ended = t == (args.n_samples*max_ep_len)-1\n",
    "\n",
    "            if terminal or epoch_ended:\n",
    "                ret_train.append(ep_ret)\n",
    "                if epoch_ended and not(terminal):\n",
    "                    print('Warning: trajectory cut off by epoch at %d steps.'%ep_len, flush=True)\n",
    "\n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                if timeout or epoch_ended:\n",
    "                    _, v, _ = actor_critic.step(o) \n",
    "                else:\n",
    "                    v = 0\n",
    "                buf.finish_path(v)\n",
    "                o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "\n",
    "        # Perform PPO update!\n",
    "        loss_pi, loss_v = update()\n",
    "\n",
    "        # Print progress\n",
    "        err = 8\n",
    "        print(\"iter \", epoch+1, \" -> loss (π): \", round(loss_pi, err), \"(v): \", round(loss_v, err),\n",
    "              \"| rewards: (train) \", round(np.mean(ret_train), err))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b018223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  1  -> loss (π):  -0.01275836 (v):  0.00564773 | rewards: (train)  0.0\n",
      "iter  2  -> loss (π):  -0.03973612 (v):  3.79e-05 | rewards: (train)  0.0\n",
      "iter  3  -> loss (π):  -0.03376961 (v):  1.026e-05 | rewards: (train)  0.0\n",
      "iter  4  -> loss (π):  -0.03195149 (v):  3.47e-06 | rewards: (train)  0.0\n"
     ]
    }
   ],
   "source": [
    "ppo(env, ac, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95512fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlpyt] *",
   "language": "python",
   "name": "conda-env-rlpyt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
